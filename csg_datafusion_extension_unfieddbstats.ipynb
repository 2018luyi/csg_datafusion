{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics (under work)\n",
    "v0.5.0\n",
    "By Stephen Karl Larroque\n",
    "License: All rights reserved (in the future will be converted to MIT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcefully autoreload all python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX FUNCTIONS\n",
    "\n",
    "import os, sys\n",
    "\n",
    "cur_path = os.path.realpath('.')\n",
    "sys.path.append(os.path.join(cur_path, 'csg_fileutil_libs'))  # for unidecode and cleanup_name, because it does not support relative paths (yet?)\n",
    "\n",
    "import re\n",
    "\n",
    "from csg_fileutil_libs.aux_funcs import save_df_as_csv, _tqdm, compute_best_diag, reorder_cols_df, find_columns_matching, cleanup_name, replace_buggy_accents, convert_to_datetype, df_drop_duplicated_index, df_to_unicode, df_to_unicode_fast, cleanup_name_df, df_literal_eval, compute_best_diag, df_unify, df_translate, df_filter_nan_str, concat_vals_unique, reorder_cols_df, sort_and_deduplicate, df_squash_lists, df_literal_eval, df_fillnastr\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)  # for reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nice plots!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# Unified database, not yet postprocessed\n",
    "unified_csv = r'databases_output\\merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall_nifti.csv'\n",
    "unifiedpersubj_csv = r'databases_output\\merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall.csv'\n",
    "output_dir = r'databases_output'\n",
    "\n",
    "# Hide null values in plots?\n",
    "plot_hide_nan = True\n",
    "\n",
    "diagorder_doc = ['', 'na', 'impossible', 'braindead', 'coma', 'vs/uws', 'mcs', 'mcs-', 'mcs+', 'srmcs', 'emcs', 'lis', 'lis_incomplete', 'partial lis']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "## PREPARE DATASET (AND ONLYDOC DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv dbs as dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cf_unified = pd.read_csv(unified_csv, sep=';', low_memory=False).dropna(axis=0, how='all').fillna('')  # drop empty lines\n",
    "cf_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified['unified.diagnosis_best'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to keep only doc patients (susceptible to being sedated)\n",
    "cf_unified_onlydoc = cf_unified[cf_unified['unified.diagnosis_best'].isin(['vs/uws', 'mcs', 'mcs+', 'mcs-', 'emcs', 'srmcs', 'coma', 'lis', 'lis_incomplete', 'partial lis', 'conflict', 'braindead'])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by name\n",
    "cf_unified_onlydoc_byname = cf_unified_onlydoc.groupby('name').agg(concat_vals_unique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check diagnoses count is fine (sanity check)\n",
    "cf_unified_onlydoc_byname.reset_index().loc[:, ['name', 'unified.diagnoses_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_df_as_csv(cf_unified_onlydoc_byname, 'onlydoc.csv', fields_order=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "## FOR MURIELLE (MRI SEDATION STATS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bynamecounts.txt', 'w') as f:\n",
    "    f.write(cf_unified_onlydoc_byname.count().to_string())\n",
    "cf_unified_onlydoc_byname.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc[cf_unified_onlydoc['nifti.func OK'].isin(['O', 'M', 'M2', 'N'])].groupby('name').agg(concat_vals_unique).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc[cf_unified_onlydoc['nifti.struct OK (for fmri)'].isin(['O', 'M', 'M2', 'N', 'W'])].groupby('name').agg(concat_vals_unique).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregate per MRI sessions\n",
    "cf_unified_onlydoc_sess = cf_unified_onlydoc[~cf_unified_onlydoc['StudyDate'].isnull() & (cf_unified_onlydoc['StudyDate'] != '')].groupby(['name', 'StudyDate']).agg(concat_vals_unique)\n",
    "cf_unified_onlydoc_sess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc_sess[~cf_unified_onlydoc_sess['nifti.func OK'].isin(['X', ''])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc_sess[~cf_unified_onlydoc_sess['nifti.struct OK (for fmri)'].isin(['X', ''])].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveepisedat(cf, appendtext=''):\n",
    "    a = cf['unified.episedation']\n",
    "    b = a.astype('str').value_counts()\n",
    "    c = b.to_frame().reset_index().rename(columns={'index': 'sedation', 'unified.episedation': 'count'})\n",
    "    #df_to_unicode_fast(c).to_excel(unified_csv[:-4] + '_episedationcount%s.xls' % appendtext)  # for python 2\n",
    "    c.to_excel(unified_csv[:-4] + '_episedationcount%s.xls' % appendtext)\n",
    "    return True\n",
    "saveepisedat(cf_unified_onlydoc_sess, '_persess')\n",
    "saveepisedat(cf_unified_onlydoc_byname, '_persubject')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "#toplot = cf_unified_perdiag[cf_unified_perdiag['unified.diagnosis_worst'] == diag]['unified.diagnosis_best'].astype('str').value_counts(dropna=plot_hide_nan)\n",
    "cf_unified_onlydoc_byname['unified.etiology'].value_counts().plot(fig=fig, kind='pie', title='Etiology of DOC patients\\n%i patients' % (cf_unified_onlydoc_byname.shape[0]), autopct='%.1f%%', figsize=(15,15))\n",
    "plt.axis('off')\n",
    "fig.savefig(os.path.join(output_dir, 'fig_docetio.png'), bbox_inches='tight', dpi=600)\n",
    "with open(os.path.join(output_dir, 'fig_docetio.txt'), 'w') as f:\n",
    "    f.write(cf_unified_onlydoc_byname['unified.etiology'].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "cf_unified_onlydoc_sess.loc[cf_unified_onlydoc_sess['unified.diagnosis_best'] == 'srmcs', 'unified.diagnosis_best'] = 'mcs+'\n",
    "for diag in cf_unified_onlydoc_sess['unified.diagnosis_best'].unique():\n",
    "    fig = plt.figure()\n",
    "    toplot = cf_unified_onlydoc_sess.loc[cf_unified_onlydoc_sess['unified.diagnosis_best'] == diag, 'unified.episedation']\n",
    "    toplot.value_counts().plot(fig=fig, kind='pie', title='Sedation for diag %s\\n%i sessions' % (diag.replace('/', '-'), toplot.shape[0]), autopct='%.1f%%', figsize=(15,15))\n",
    "    plt.axis('off')\n",
    "    fig.savefig(os.path.join(output_dir, 'fig_sedat_%s.png' % diag.replace('/', '-')), bbox_inches='tight', dpi=600)\n",
    "    with codecs.open(os.path.join(output_dir, 'fig_sedat_%s.txt' % diag.replace('/', '-')), 'w', 'utf-8-sig') as f:\n",
    "        f.write(toplot.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "## MARKOV CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv dbs as dataframes\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "cf_unifiedsubj = pd.read_csv(unifiedpersubj_csv, sep=';', low_memory=False).dropna(axis=0, how='all').fillna('')  # drop empty lines\n",
    "cf_unifiedsubj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unifiedsubj['unified.diagnosis_best'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter to keep only doc patients (susceptible to being sedated)\n",
    "cf_unifiedsubj_onlydoc = cf_unifiedsubj[cf_unifiedsubj['unified.diagnosis_best'].isin(['vs/uws', 'mcs', 'mcs+', 'mcs-', 'emcs', 'srmcs', 'coma', 'lis', 'lis_incomplete', 'partial lis', 'conflict', 'braindead'])]\n",
    "cf_unifiedsubj_onlydoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc_byname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc_byname.reset_index().loc[:, ['name', 'unified.diagnoses_count']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_columns_matching(cf_unified_onlydoc_byname, ['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract max crsr count\n",
    "cf_unified_onlydoc_byname['unified.diagnoses_count_withdate'] = cf_unified_onlydoc_byname['unified.diagnoses_count_withdate'].apply(lambda x: max(x) if isinstance(x, list) else x)\n",
    "cf_unified_onlydoc_byname['unified.diagnoses_count_withdate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diagcount = df_squash_lists(cf_unified_onlydoc_byname['unified.diagnoses_count'], func=max, aggressive=True)\n",
    "diagcount.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show number of patients with at least 1, then 2 CRS-R but not necessarily with date\n",
    "diagcount = df_squash_lists(cf_unified_onlydoc_byname['unified.diagnoses_count'], func=max, aggressive=True).astype('int')\n",
    "diagcountwithdate = df_squash_lists(cf_unified_onlydoc_byname['unified.diagnoses_count_withdate'], func=max, aggressive=True).astype('float')\n",
    "print('At least one CRS-R but not necessarily with dates:')\n",
    "print((diagcount >= 1).sum())\n",
    "print('At least two CRS-R but not necessarily with dates:')\n",
    "print((diagcount >= 2).sum())\n",
    "print('At least one CRS-R with date:')\n",
    "print((diagcountwithdate >= 1).sum())\n",
    "print('At least two CRS-R with dates:')\n",
    "print((diagcountwithdate >= 2).sum())\n",
    "print('Both worst and best diags have dates:')\n",
    "print(((~cf_unified_onlydoc_byname['unified.diagnoses_firstbestdiag'].isnull()) & (~cf_unified_onlydoc_byname['unified.diagnoses_firstworstdiag'].isnull()) & (diagcountwithdate >= 2)).sum())\n",
    "print('Both worst and best diags have dates OR best diag is LIS or lis incomplete:')\n",
    "doc2diagdatepluslis = \\\n",
    "    (\n",
    "    # at least 2 diagnoses with both best diag and worst diag having dates info\n",
    "    ((~cf_unified_onlydoc_byname['unified.diagnoses_firstbestdiag'].isnull()) & (~cf_unified_onlydoc_byname['unified.diagnoses_firstworstdiag'].isnull()) & (diagcountwithdate >= 2))\n",
    "    # or it's a LIS, then no need for dates (because anyway there's none, there is no CRS-R)\n",
    "    | (df_squash_lists(cf_unified_onlydoc_byname['unified.diagnosis_best'], aggressive=True).isin(['lis', 'lis_incomplete']))\n",
    "    )\n",
    "print(doc2diagdatepluslis.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find patients where the best diagnosis is a list (should not happen but it happens ???)\n",
    "cf_unified_onlydoc_byname.loc[cf_unified_onlydoc_byname['unified.diagnosis_best'].apply(lambda x: isinstance(x, list)), 'unified.diagnosis_best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only patients with both best and worst diagnoses with dates (else can't see any transition)\n",
    "cf_unified_onlydoc_byname_min2diag = cf_unified_onlydoc_byname.loc[doc2diagdatepluslis, :]\n",
    "# Also get all patients with at least two CRS-Rs but not necessarily with dates. By nature this is less precise as we can't know the direction (improvement, fluctuation or worsening) if we don't have the dates\n",
    "cf_unified_onlydoc_byname_min2diagunprecise = cf_unified_onlydoc_byname.loc[diagcount >= 2, :]\n",
    "# Drop the 'test test' patient\n",
    "cf_unified_onlydoc_byname_min2diag.drop('test test', inplace=True)\n",
    "cf_unified_onlydoc_byname_min2diagunprecise.drop('test test', inplace=True)\n",
    "# Show\n",
    "cf_unified_onlydoc_byname_min2diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out false patients names\n",
    "falsepatientsnames = [\n",
    "    'prise en charge',\n",
    "    'ant la',\n",
    "    'association',\n",
    "    'locked in',\n",
    "    'consciousness',\n",
    "    'cerebral',\n",
    "    'references',\n",
    "    'qualite de',\n",
    "    'durees',\n",
    "    'enfants et les',\n",
    "    'somatosensory',\n",
    "    'such reposiiories',\n",
    "    'evaluation',\n",
    "    'regressing',\n",
    "    'locked-in',\n",
    "    'two millennia',\n",
    "    'neuroimagerie',\n",
    "    'prognosis',\n",
    "    'traumatic',\n",
    "    'italie et',\n",
    "    'a completer',\n",
    "    'acute and subacute',\n",
    "    'ant les ',\n",
    "    'defined by',\n",
    "    'diagnosis'\n",
    "    'ant votre ',\n",
    "    'ant son ',\n",
    "    'ant leurs ',\n",
    "    ]\n",
    "cf_unified_onlydoc_byname_min2diag.drop(cf_unified_onlydoc_byname_min2diag.index[cf_unified_onlydoc_byname_min2diag.index.str.contains(r'(?:{})'.format('|'.join(falsepatientsnames)))], inplace=True)\n",
    "cf_unified_onlydoc_byname_min2diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert dates count to an int, will be easier to process (and cleaner figures)\n",
    "cf_unified_onlydoc_byname_min2diag.loc[cf_unified_onlydoc_byname_min2diag['unified.diagnoses_count_withdate'].isnull(), 'unified.diagnoses_count_withdate'] = 0  # replace nan values\n",
    "cf_unified_onlydoc_byname_min2diag.loc[:, 'unified.diagnoses_count_withdate'] = cf_unified_onlydoc_byname_min2diag['unified.diagnoses_count_withdate'].astype('int')\n",
    "cf_unified_onlydoc_byname_min2diagunprecise.loc[:, 'unified.diagnoses_count_withdate'] = cf_unified_onlydoc_byname_min2diagunprecise['unified.diagnoses_count_withdate'].astype('int')\n",
    "cf_unified_onlydoc_byname_min2diag['unified.diagnoses_count_withdate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc_byname_min2diag['unified.diagnoses_count_withdate'].plot(kind='hist', bins=max(cf_unified_onlydoc_byname_min2diag['unified.diagnoses_count_withdate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show cases where there are multiple best or worst diagnoses (which should not happen)\n",
    "conflictdiags = cf_unified_onlydoc_byname_min2diag.loc[cf_unified_onlydoc_byname_min2diag['unified.diagnosis_worst'].apply(lambda x: isinstance(x, list)), :].index\n",
    "cf_unified_onlydoc_byname_min2diag.loc[conflictdiags, find_columns_matching(cf_unified_onlydoc_byname_min2diag, 'unified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix cases where there are multiple best/worst diagnoses, by selecting the best/worst diagnosis respectively\n",
    "\n",
    "# Order diagnoses using Pandas discrete categories, so that we can easily grade the maximum and minimum diagnoses\n",
    "cf_unified_onlydoc_byname_min2diag.loc[:, 'unified.diagnosis_worst'] = cf_unified_onlydoc_byname_min2diag['unified.diagnosis_worst'].apply(lambda x: compute_best_diag(x, diag_order=diagorder_doc, persubject=None).min() if not isinstance(x, str) and x is not None else x)\n",
    "cf_unified_onlydoc_byname_min2diag.loc[:, 'unified.diagnosis_best'] = cf_unified_onlydoc_byname_min2diag['unified.diagnosis_best'].apply(lambda x: compute_best_diag(x, diag_order=diagorder_doc, persubject=None).max() if not isinstance(x, str) and x is not None else x)\n",
    "# Same for unprecise dataframe\n",
    "cf_unified_onlydoc_byname_min2diagunprecise.loc[:, 'unified.diagnosis_worst'] = cf_unified_onlydoc_byname_min2diagunprecise['unified.diagnosis_worst'].apply(lambda x: compute_best_diag(x, diag_order=diagorder_doc, persubject=None).min() if not isinstance(x, str) and x is not None else x)\n",
    "cf_unified_onlydoc_byname_min2diagunprecise.loc[:, 'unified.diagnosis_best'] = cf_unified_onlydoc_byname_min2diagunprecise['unified.diagnosis_best'].apply(lambda x: compute_best_diag(x, diag_order=diagorder_doc, persubject=None).max() if not isinstance(x, str) and x is not None else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check if the previous docs with conflicting diagnoses are now ok\n",
    "cf_unified_onlydoc_byname_min2diag.loc[conflictdiags, find_columns_matching(cf_unified_onlydoc_byname_min2diag, 'unified')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = cf_unified_onlydoc_byname_min2diag.loc[cf_unified_onlydoc_byname_min2diag['unified.diagnosis_worst'].apply(lambda x: isinstance(x, list)), find_columns_matching(cf_unified_onlydoc_byname_min2diag, 'unified')]\n",
    "# correct:\n",
    "#print(a['unified.diagnosis_worst'].apply(lambda x: compute_best_diag(x, diag_order=['', 'na', 'impossible'] + diagorder_doc + ['lis'], persubject=None).min()))\n",
    "#print(a['unified.diagnosis_worst'].apply(lambda x: compute_best_diag(x, diag_order=['', 'na', 'impossible'] + diagorder_doc + ['lis'], persubject=None).max()))\n",
    "# wrong:\n",
    "#print(a['unified.diagnosis_worst'].apply(lambda x: min(compute_best_diag(x, diag_order=['', 'na', 'impossible'] + diagorder_doc + ['lis'], persubject=None)))\n",
    "#print(a['unified.diagnosis_worst'].apply(lambda x: max(compute_best_diag(x, diag_order=['', 'na', 'impossible'] + diagorder_doc + ['lis'], persubject=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valsorder = ['AAA', 'BBBBBBBBB', 'CCCCC', 'DD', 'EEE']\n",
    "#s = pd.Series(valsorder[1:4])\n",
    "#s = s.astype(pd.api.types.CategoricalDtype(categories=valsorder, ordered=True))\n",
    "#min(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without dates transition matrix (so that we can't know in what direction the transition happened), but like this we have also non-doc such as lis, which don't have CRS-R (and no dates), only the diagnosis\n",
    "def calc_transition_matrix(df, col1, col2):\n",
    "    \"\"\"Compute the transition matrix between two columns containing categorical values (but not necessarily CategoricalDTypes) in a pandas DataFrame.\n",
    "    Returns two dataframes: the first with probabilities, the second with counts\"\"\"\n",
    "    try:\n",
    "        tmat = pd.DataFrame(0, index=df[col1].unique(), columns=df[col2].unique())\n",
    "    except TypeError as exc:\n",
    "        tmat = pd.DataFrame(0, index=df[col1].astype('str').unique(), columns=df[col2].astype('str').unique())\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        # If worst diag is empty, then it's probably a LIS (no CRS-R, only one diagnosis), then we take it as a no change\n",
    "        if row[col1] is None or row[col1] == '':\n",
    "            tmat.loc[row[col2], row[col2]] += 1\n",
    "        else:\n",
    "            tmat.loc[row[col1], row[col2]] += 1\n",
    "    \n",
    "    # Compute probabilities\n",
    "    tmatcount = tmat.copy()\n",
    "    tmat = tmat.apply(lambda x: x / x.sum(), axis=1)\n",
    "    # Return results\n",
    "    return tmat, tmatcount\n",
    "\n",
    "tmat, tmatcount = calc_transition_matrix(cf_unified_onlydoc_byname_min2diag, 'unified.diagnosis_worst', 'unified.diagnosis_best')\n",
    "tmatu, tmatucount = calc_transition_matrix(cf_unified_onlydoc_byname_min2diagunprecise, 'unified.diagnosis_worst', 'unified.diagnosis_best')\n",
    "tmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmatcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(tmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns and indices\n",
    "def reordertransitionmatrix(df, orderlist):\n",
    "    df2 = df.copy()\n",
    "    df2 = df2.loc[:, [x for x in orderlist if x in df2.columns]]  # easiest way: get the whole ordered list and filter it through the existing columns\n",
    "    df2 = df2.loc[[x for x in orderlist if x in df2.index], :]\n",
    "    return df2\n",
    "\n",
    "tmat = reordertransitionmatrix(tmat, diagorder_doc)\n",
    "tmatcount = reordertransitionmatrix(tmatcount, diagorder_doc)\n",
    "tmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tmatcount.sum().sum())\n",
    "tmatcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without ensuring that both dates are present, we just know that there are at least 2 CRS-R diagnoses\n",
    "# There are 200 more subjects like this, but not sure they are all correct (some diagnoses might have been duplicated)\n",
    "# TODO: add back the LIS also in this dataset if I use it!\n",
    "tmatu = reordertransitionmatrix(tmatu, diagorder_doc)\n",
    "tmatucount = reordertransitionmatrix(tmatucount, diagorder_doc)\n",
    "print(tmatucount.sum().sum())\n",
    "tmatucount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotheatmap(df):\n",
    "    df2 = df.copy()  # make a copy of the dataframe, avoids side effects\n",
    "    df2[df2==0] = float('NaN')  # make 0 values blank\n",
    "    plt.pcolor(df2, cmap=plt.get_cmap('viridis'))\n",
    "    plt.yticks(np.arange(0.5, len(df2.index), 1), df2.index)\n",
    "    plt.xticks(np.arange(0.5, len(df2.columns), 1), df2.columns)\n",
    "    plt.show()\n",
    "\n",
    "plotheatmap(tmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transition matrix for at least 5 CRS-R (with at least worst and best with dates info)\n",
    "tmat5, tmat5count = calc_transition_matrix(cf_unified_onlydoc_byname_min2diag.loc[cf_unified_onlydoc_byname_min2diag['unified.diagnoses_count_withdate'] >= 5, :], 'unified.diagnosis_worst', 'unified.diagnosis_best')\n",
    "tmat5 = reordertransitionmatrix(tmat5, diagorder_doc)\n",
    "tmat5count = reordertransitionmatrix(tmat5count, diagorder_doc)\n",
    "print(tmat5)\n",
    "print(tmat5count)\n",
    "plotheatmap(tmat5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RESULT: compare tmat5 (at least 5 CRS-R, down) to tmat (at least 2 CRS-Rs, up figure), it suggests that mcs- might be most of the time a misdiagnosis of mcs+. Same for mcs+, misdiagnosis of emcs. srmcs always changes to emcs, so the criterion requiring 2 consecutive is not necessary, but we have a small sample. This suggests modifications to guidelines (READ REF giacino guidelines).\n",
    "plotheatmap(tmat)\n",
    "plotheatmap(tmat5)\n",
    "print(tmat)\n",
    "print(tmat5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_transition_matrix_totals(df, colsnames=None):\n",
    "    '''Compute total sum over each row of a square matrix: sum of what is left to current index, what is on self node, and sum on the right'''\n",
    "    res = pd.DataFrame(index=df.index, columns=['left', 'self', 'right'])\n",
    "    for idx, row in df.iterrows():\n",
    "        res.loc[idx, 'left'] = row[:row.index.get_loc(idx)].sum()\n",
    "        if row.index.get_loc(idx) < len(row.index):\n",
    "            res.loc[idx, 'right'] = row[row.index.get_loc(idx)+1:].sum()\n",
    "        res.loc[idx, 'self'] = row[idx]\n",
    "    # Rename columns if supplied\n",
    "    if colsnames is not None and len(colsnames) == 3:\n",
    "        res.columns = colsnames\n",
    "    # Return\n",
    "    return res\n",
    "\n",
    "def calc_transition_matrix_withdates(df_in, diagorder, col1, col2, col1datefirst, col2datefirst, col1datelast, col2datelast):\n",
    "    \"\"\"Compute the transition matrix between two columns containing categorical values (but not necessarily CategoricalDTypes) in a pandas DataFrame.\n",
    "    This takes into account the dates, so that the transition matrix will be directional. This also allows to compute the totals.\n",
    "    col1 is the initial state, col2 the resulting state.\n",
    "    Returns two dataframes: the first with probabilities, the second with counts\"\"\"\n",
    "\n",
    "    # Copy to avoid side effects\n",
    "    df = df_in.copy()\n",
    "\n",
    "    # Prepare the input columns to date format\n",
    "    df.loc[:, col1datefirst] = convert_to_datetype(df_squash_lists(df[col1datefirst], lambda x: x[0]), col1datefirst, '%Y-%m-%d').set_index(df.index)\n",
    "    df.loc[:, col2datefirst] = convert_to_datetype(df_squash_lists(df[col2datefirst], lambda x: x[0]), col2datefirst, '%Y-%m-%d').set_index(df.index)\n",
    "    df.loc[:, col1datelast] = convert_to_datetype(df_squash_lists(df[col1datelast], lambda x: x[-1]), col1datelast, '%Y-%m-%d').set_index(df.index)\n",
    "    df.loc[:, col2datelast] = convert_to_datetype(df_squash_lists(df[col2datelast], lambda x: x[-1]), col2datelast, '%Y-%m-%d').set_index(df.index)\n",
    "\n",
    "    # Initialize the transition matrix with the appropriate columns and indices\n",
    "    try:\n",
    "        tmat = pd.DataFrame(0, index=df[col1].unique().tolist(), columns=df[col2].unique().tolist())\n",
    "    except TypeError as exc:\n",
    "        tmat = pd.DataFrame(0, index=df[col1].astype('str').unique().tolist(), columns=df[col2].astype('str').unique().tolist())\n",
    "\n",
    "    # Reorder the columns/indices (the order is important to compute the totals later)\n",
    "    #tmat = reordertransitionmatrix(tmat, diagorder+totalcols)\n",
    "    tmat = reordertransitionmatrix(tmat, diagorder)\n",
    "\n",
    "    # Initialize the fluctuation matrix\n",
    "    tmatfluc = tmat.copy()\n",
    "\n",
    "    # Build the transition matrix by incrementing counters\n",
    "    for idx, row in df.iterrows():\n",
    "        # If worst diag is empty, then it's probably a LIS (no CRS-R, only one diagnosis), then we take it as a no change\n",
    "        if row[col1] is None or row[col1] == '':\n",
    "            tmat.loc[row[col2], row[col2]] += 1\n",
    "        # Else it's the same diagnosis for worst and best, this is stable, we simply add it\n",
    "        elif row[col1] == row[col2]:\n",
    "            tmat.loc[row[col1], row[col2]] += 1\n",
    "        # Else both diagnoses are different, we need to know if it's an improvement or a worsening or even a fluctuation, we will use the date info to disambiguate\n",
    "        else:\n",
    "            # Calculate delay between the last worst diagnosis and the last best diagnosis\n",
    "            deltalastworst2lastbest = (row[col2datelast] - row[col1datelast]).days\n",
    "            # Switch to appropriate direction depending on the value of this delay\n",
    "            if deltalastworst2lastbest > 0:\n",
    "                # Positive delay: last worst diagnosis -> last best diagnosis: the last best diagnosis happened later than the last worst diagnosis, apriori it's an improvement...\n",
    "                # ...but just in case, check if maybe the best diagnosis also appeared first before the last worst diagnosis, which would mean that it's fluctuating between both\n",
    "                deltafirstbest2lastworst = (row[col1datelast] - row[col2datefirst]).days\n",
    "                if deltafirstbest2lastworst <= 0:\n",
    "                    # Negative or null delay: the first best diagnosis happened after the last worst diagnosis, so this confirms that it's NOT a fluctuation but an improvement\n",
    "                    tmat.loc[row[col1], row[col2]] += 1\n",
    "                else:\n",
    "                    # Positive delay: this confirms this is a fluctuation: there was a temporal sequence of best diagnosis -> worst diagnosis -> best diagnosis, so that's clearly a fluctuation\n",
    "                    tmatfluc.loc[row[col1], row[col2]] += 1\n",
    "            elif deltalastworst2lastbest < 0:\n",
    "                # Negative delay: last best diagnosis -> last worst diagnosis. So this is a worsening apriori...\n",
    "                # ...but we also check if maybe there was a sequence like worst diagnosis -> best diagnosis -> worst diagnosis, indicative of a fluctuation.\n",
    "                deltafirstworst2lastbest = (row[col2datelast] - row[col1datefirst]).days\n",
    "                if deltafirstworst2lastbest <= 0:\n",
    "                    # Negative or null delay: this confirms this is a worsening, not a fluctuation\n",
    "                    tmat.loc[row[col2], row[col1]] += 1\n",
    "                else:\n",
    "                    # Positive delay: there is indeed a worst diagnosis before the last best diagnosis, so this is a fluctuation\n",
    "                    tmatfluc.loc[row[col1], row[col2]] += 1\n",
    "            elif deltalastworst2lastbest == 0:\n",
    "                # Null delay: both the last best and last worst diagnoses happened on the same day, so we can't know for sure what happened (we don't have the exact timing, the day is the highest resolution we have), so we assume a fluctuation (just to be same)\n",
    "                tmatfluc.loc[row[col1], row[col2]] += 1\n",
    "    \n",
    "    # Compute totals\n",
    "    totalcols = ['worsening', 'nochange', 'improvement']\n",
    "    tmattot = compute_transition_matrix_totals(tmat, totalcols)\n",
    "    tmatfluctot = compute_transition_matrix_totals(tmatfluc, totalcols)\n",
    "\n",
    "    # Return results\n",
    "    return tmat, tmattot, tmatfluc, tmatfluctot\n",
    "\n",
    "def transition_matrix_to_proba(tmat):\n",
    "    '''Compute probabilities from a transition matrix containing the count occurrences of each transitions'''\n",
    "    return tmatproba.apply(lambda x: x / x.sum(), axis=1)\n",
    "\n",
    "calc_transition_matrix_withdates(cf_unified_onlydoc_byname_min2diag, diagorder_doc,\n",
    "                                 'unified.diagnosis_worst', 'unified.diagnosis_best',\n",
    "                                 'unified.diagnoses_firstworstdiag', 'unified.diagnoses_firstbestdiag',\n",
    "                                 'unified.diagnoses_lastworstdiag', 'unified.diagnoses_lastbestdiag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIMITATIONS OF THIS STUDY:\n",
    "# * does not account for temporality between worst and best diagnosis, thus worst diagnosis may well be an evolution happening later than the best diagnosis. Here we show the possible transitions between both, should be considered bidirectional. Thus interpretation is not necessarily of an evolution but a possible transition between both states.\n",
    "# we could change that but what criterion should we use? And what timeframe, if it's a daytoday fluctuation, should we consider this is ...? Or simply restrict analysis to all crs-r timeframe under 3 months, so we consider it's not evolution, only fluctuation or short term evolution.\n",
    "\n",
    "# RESULTS\n",
    "#* most change diag, dont be fooled by the heatmap, so this and graph are bad viz, they dont show the main result. Problem with heatmap is the colors: how do you add the colors to know that in fact where it's most salient isn't the majority of the changes?\n",
    "#* SOLUTION: add 3 columns: worsening, no change and improvement, and these will be the sum of enhancement vs no change vs worsening. Simple to calculate: same position in x and y = no change, below position in columns compared to index = worsening, opposite is improvement.\n",
    "#* srmcs 50% chance change to emcs. we question the pertinence of requiring 2 consecutive fulfillment of the tasks\n",
    "\n",
    "# TODO:\n",
    "#* account for bidirectionality by detecting order of worst and best diag?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division  # Only for how I'm writing the transition matrix\n",
    "import networkx as nx  # For the magic\n",
    "import matplotlib.pyplot as plt  # For plotting\n",
    "\n",
    "# Install pydot and graphviz beforehand, and change the path below on Windows to your graphviz folder\n",
    "\n",
    "# Add graphviz to the path, so that the binaries can be found (if you get an error about dot not being found, that's why, modify this path here AND also you need nxpydot and pydotplus installed)\n",
    "os.environ[\"PATH\"] += os.pathsep + r'C:/Program Files (x86)/Graphviz2.38/bin'\n",
    "\n",
    "# and the following code block is not needed\n",
    "# but we want to see which module is used and\n",
    "# if and why it fails\n",
    "try:\n",
    "    import pydotplus # also requires nxpydot installed but no need to import\n",
    "    from networkx.drawing.nx_pydot import write_dot, to_pydot\n",
    "    print(\"using package pydotplus\")\n",
    "except ImportError:\n",
    "    print()\n",
    "    print(\"Module pydotplus and nxpydot were not found (nxpydot is a wrapper for networkx to use pydotplus, please uninstall pydot beforehand as it is incompatible with python 3) \")\n",
    "    print(\"see https://networkx.github.io/documentation/latest/reference/drawing.html\")\n",
    "    print()\n",
    "    raise\n",
    "\n",
    "def transition_to_graph(df):\n",
    "    # Adapted from https://vknight.org/unpeudemath/code/2015/11/15/Visualising-markov-chains.html\n",
    "    G = nx.MultiDiGraph(directed=True)\n",
    "    labels={}\n",
    "    edge_labels={}\n",
    "\n",
    "    for state1 in df.index:\n",
    "        for state2 in df.columns:\n",
    "            weight = df.loc[state1, state2]\n",
    "            if weight > 0:\n",
    "                G.add_edge(state1,\n",
    "                           state2,\n",
    "                           weight=weight,\n",
    "                           penwidth=weight*10,\n",
    "                           label=\"{:.02f}\".format(weight))\n",
    "                edge_labels[(state1, state2)] = label=\"{:.02f}\".format(weight)\n",
    "    return G\n",
    "\n",
    "def plot_transition_graph(G, pos=None):\n",
    "    # Plot using networkx internal visualization with matplotlib, it does not support multiple directed edges (use graphviz instead)\n",
    "    # https://stackoverflow.com/questions/20133479/how-to-draw-directed-graphs-using-networkx-in-python\n",
    "    plt.figure(figsize=(14,7))\n",
    "    #node_size = 200\n",
    "    #pos = {state:list(state) for state in states}\n",
    "    #nx.draw_networkx_edges(G,pos,width=1.0,alpha=0.5)\n",
    "    #nx.draw_networkx_labels(G, pos, font_weight=2)\n",
    "    options = {\n",
    "        'node_color': 'cyan',\n",
    "        'node_size': 2000,\n",
    "        'width': 1,\n",
    "        'arrowstyle': '-|>',\n",
    "        'arrowsize': 30,\n",
    "    }\n",
    "    if pos is None:\n",
    "        # Get the layout defined manually in G\n",
    "        pos = nx.get_node_attributes(G,'pos')\n",
    "        if not pos:\n",
    "            # Else calculate a layout automatically\n",
    "            #pos = nx.nx_pydot.graphviz_layout(G, prog='neato')\n",
    "            pos = nx.drawing.layout.spectral_layout(G)\n",
    "    nx.draw(G, pos, arrows=True, with_labels=True, **options)\n",
    "    labels = nx.get_edge_attributes(G, 'weight')\n",
    "    nx.draw_networkx_edge_labels(G, pos, edge_labels=labels)\n",
    "    plt.axis('off')\n",
    "\n",
    "def plot_transition_graph2(G, pos=None):\n",
    "    # In-memory plotting using GraphViz (supports multiple directed edges between two nodes), pydotplus and nxpydot via networkx\n",
    "    from io import BytesIO\n",
    "    import matplotlib.image as mpimg\n",
    "    # add graphviz layout options (see https://stackoverflow.com/a/39662097)\n",
    "    G.graph['edge'] = {'arrowsize': '0.6', 'splines': 'curved', 'rankdir':'LR'}\n",
    "    G.graph['graph'] = {'scale': '3'}\n",
    "\n",
    "    # adding attributes to edges in multigraphs is more complicated but see\n",
    "    # https://stackoverflow.com/a/26694158\n",
    "    #G[1][1][0]['color']='red'\n",
    "\n",
    "    # From: https://stackoverflow.com/questions/4596962/display-graph-without-saving-using-pydot\n",
    "    # convert from networkx -> pydot\n",
    "    pydot_graph = to_pydot(G)\n",
    "\n",
    "    # render pydot by calling dot, no file saved to disk\n",
    "    png_str = pydot_graph.create_png(prog='dot') # can change to dot or twopi, but not neato because the latter is only for non directed graphs\n",
    "\n",
    "    # treat the dot output string as an image file\n",
    "    sio = BytesIO()\n",
    "    sio.write(png_str)\n",
    "    sio.seek(0)\n",
    "    img = mpimg.imread(sio)\n",
    "\n",
    "    # plot the image\n",
    "    plt.figure(figsize=(40,15))\n",
    "    imgplot = plt.imshow(img, aspect='equal')\n",
    "    plt.axis('off')\n",
    "    plt.show(block=False)\n",
    "\n",
    "G = transition_to_graph(tmat)\n",
    "# set position manually\n",
    "#for i, n in enumerate(G):\n",
    "#    G.node[n]['pos'] = '\"%d,%d\"' % (i, 1)\n",
    "write_dot(G, 'mc.dot')\n",
    "#plot_transition_graph2(G) # TODO: fix this (or just use next cell?), it now fails but dunno why"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In-memory plotting with better settings\n",
    "from io import BytesIO\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# convert from networkx -> pydot\n",
    "pydot_graph = to_pydot(G)\n",
    "pydot_graph.set_concentrate(True)\n",
    "pydot_graph.set_layout('dot')\n",
    "pydot_graph.set_dpi(300)\n",
    "pydot_graph.set_pack(True)\n",
    "#pydot_graph.set_rank('same')\n",
    "pydot_graph.set_splines('line')\n",
    "\n",
    "# render pydot by calling dot, no file saved to disk\n",
    "png_str = pydot_graph.create_png(prog='dot') # can change to dot or twopi, but not neato because the latter is only for non directed graphs\n",
    "\n",
    "# treat the dot output string as an image file\n",
    "sio = BytesIO()\n",
    "sio.write(png_str)\n",
    "sio.seek(0)\n",
    "img = mpimg.imread(sio)\n",
    "\n",
    "# plot the image\n",
    "plt.figure(figsize=(40,15))\n",
    "imgplot = plt.imshow(img, aspect='equal')\n",
    "plt.axis('off')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_unified_onlydoc_byname_min2diag['unified.diagnosis_best'].astype('str').unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_columns_matching(cf_unified_onlydoc_byname_min2diag, 'unified')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------\n",
    "## Predictive model with machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the columns we want to build the predictive model\n",
    "mlcols = ['unified.diagnosis_best', 'unified.diagnosis_worst', 'unified.diagnoses_timeperiod',\n",
    "          'unified.diagnoses_count', 'unified.age', 'unified.gender', 'unified.acute', 'unified.etiology']\n",
    "\n",
    "cf_ml = cf_unified_onlydoc_byname_min2diag.reset_index().loc[:, mlcols]\n",
    "cf_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Squash lists so that we have only one value per feature per sample\n",
    "from collections import Counter\n",
    "\n",
    "def calcmode(nparr):\n",
    "    # From: https://stackoverflow.com/questions/16330831/most-efficient-way-to-find-mode-in-numpy-array\n",
    "    # There is also a multi-dimensional version available\n",
    "    # in worst case, return the min\n",
    "    mode = Counter(nparr).most_common(1)\n",
    "    # mode will be [(6,3)] to give the count of the most occurring value, so ->\n",
    "    m = mode[0][0]\n",
    "    return m\n",
    "\n",
    "cf_ml = cf_ml.apply(lambda x: df_squash_lists(x, func=calcmode, aggressive=True))\n",
    "cf_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timeperiod to ints (representing the days) and then a logarithm transform\n",
    "# Alternatives: https://stackoverflow.com/questions/9775743/how-can-i-parse-free-text-time-intervals-in-python-ranging-from-years-to-second\n",
    "#datetime.strptime(cf_ml['unified.diagnoses_timeperiod'][0], '%d days %H:%M:%S.%f000').day\n",
    "\n",
    "# Get the number of days from the timedelta string\n",
    "cf_ml['unified.diagnoses_timeperiod'] = cf_ml['unified.diagnoses_timeperiod'].str.extract('\\s*(\\d+)\\s+').astype('float')\n",
    "# Convert to logarithm\n",
    "cf_ml['unified.diagnoses_timeperiod'] = cf_ml['unified.diagnoses_timeperiod'].apply(lambda x: np.log(x))\n",
    "# Since we use log, 0 will become -inf, so we restore back the 0 value for those (which means that all CRS-R assessments happened on the same day)\n",
    "cf_ml.loc[cf_ml['unified.diagnoses_timeperiod'] == float('-inf'), 'unified.diagnoses_timeperiod'] = 0.0\n",
    "# Show result\n",
    "cf_ml['unified.diagnoses_timeperiod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up age and convert to float (so that it's not a categorical feature but an ordinal, with an order)\n",
    "cf_ml.loc[cf_ml['unified.age'].isnull() | (cf_ml['unified.age'] == 'None') | (cf_ml['unified.age'] == '#VALUE!'), 'unified.age'] = float('NaN')\n",
    "cf_ml['unified.age'] = cf_ml['unified.age'].astype('float')\n",
    "cf_ml['unified.age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which subjects have very old age\n",
    "# You'll have to fix them manually if there are errors\n",
    "agecheck = df_fillnastr(df_squash_lists(cf_unified_onlydoc_byname_min2diag['unified.age'], func=calcmode, aggressive=True), float('NaN')).astype('float')\n",
    "cf_unified_onlydoc_byname_min2diag.loc[(agecheck < 15) | (agecheck > 90), ['unified.age'] + find_columns_matching(cf_unified_onlydoc_byname_min2diag, 'unified')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_ml[(cf_ml['unified.age'] < 15) | (cf_ml['unified.age'] > 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix manually age issues\n",
    "cf_ml.loc[152, 'unified.age'] = 33.0\n",
    "cf_ml.loc[250, 'unified.age'] = 22.0\n",
    "cf_ml.loc[410, 'unified.age'] = 31.0\n",
    "cf_ml.loc[2, 'unified.age'] = 57.0\n",
    "cf_ml.loc[120, 'unified.age'] = 38.0\n",
    "cf_ml.loc[125, 'unified.age'] = 42.0\n",
    "cf_ml.loc[175, 'unified.age'] = 24.0\n",
    "\n",
    "# Check they disapper\n",
    "cf_ml[(cf_ml['unified.age'] < 15) | (cf_ml['unified.age'] > 90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_ml['unified.etiology'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert diagnoses count to int type\n",
    "# Note: use df_fillnastr in case of issues with nan values\n",
    "cf_ml['unified.diagnoses_count'] = cf_ml['unified.diagnoses_count'].astype('int')\n",
    "cf_ml['unified.diagnoses_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing data plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import missingno as msno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in cf_ml.columns:\n",
    "    print('= Values for column %s (first without and then with log scale):' % col)\n",
    "    if str(cf_ml[col].dtype).startswith('float') or str(cf_ml[col].dtype).startswith('int'):\n",
    "        # Without log\n",
    "        plt.hist(cf_ml[col], range=[cf_ml[col].min(), cf_ml[col].max()], bins=20)\n",
    "        plt.show()\n",
    "        # With log\n",
    "        plt.hist(cf_ml[col], range=[cf_ml[col].min(), cf_ml[col].max()], bins=20, log=True)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(cf_ml[col].value_counts())\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert null strings in all str and object to real None object so that missingno can detect them\n",
    "def fillfakena(df_in):\n",
    "    '''Convert null strings in all str and object to real None object so that missingno can detect them'''\n",
    "    df = df_in.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'str' or df[col].dtype == 'object' or df[col].dtype.kind == 'O':\n",
    "            df[col] = df_fillnastr(df[col], None)\n",
    "    return df\n",
    "\n",
    "cf_ml = fillfakena(cf_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(cf_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.bar(cf_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No real pairwise missingness correlation, only shows that for patients where more infos filled, the data is more complete. So when the operator took the time to do a proper job.\n",
    "msno.heatmap(cf_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# age and acute not so much related here, it's the last merge before final\n",
    "msno.dendrogram(cf_ml)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation for machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert back null values to null strings, making them their own class (separate class strategy for handling missing values)\n",
    "def fillnaseparateclass(df_in):\n",
    "    '''Convert back null values to null strings, making them their own class (separate class strategy for handling missing values)'''\n",
    "    df = df_in.copy()\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'str' or df[col].dtype == 'object' or df[col].dtype.kind == 'O':\n",
    "            df.loc[df[col].isnull(), col] = 'None'\n",
    "    return df\n",
    "\n",
    "cf_ml = fillnaseparateclass(cf_ml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dataset by separating y class from X features\n",
    "X_orig = cf_ml.drop(columns='unified.diagnosis_best')\n",
    "y_orig = cf_ml['unified.diagnosis_best']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of categorical columns\n",
    "categorical_columns = X_orig.select_dtypes(exclude=['int','float']).columns\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data splitting (test dataset, cross-validation dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split to get a test set, and separate X from y (target class) but by balancing to keep same proportions in both relatively to y\n",
    "# Although balance is not strictly guaranted: https://github.com/scikit-learn/scikit-learn/issues/8913\n",
    "from sklearn import model_selection\n",
    "\n",
    "splitter = model_selection.StratifiedShuffleSplit(n_splits=1, test_size=0.1, train_size=None, random_state=0)\n",
    "train_idx, test_idx = next(splitter.split(X_orig, y_orig))\n",
    "X_train, X_test = X_orig.loc[train_idx, :], X_orig.loc[test_idx, :]\n",
    "y_train, y_test = y_orig[train_idx], y_orig[test_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data augmentation - balanced learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oversample randomly (duplicating rows as-is) to balance classes\n",
    "oversample = True\n",
    "\n",
    "def random_oversample(X, y):\n",
    "    '''Random oversampling by non majority selection: the classes with less samples will be duplicated.\n",
    "    This is an agnostic alternative to imbalanced-learn package which does not support nan/null/None values nor unencoded categorical variables.'''\n",
    "\n",
    "    if not (y.value_counts() != y.value_counts().max()).any():\n",
    "        # All classes are already balanced, nothing to do\n",
    "        return X, y\n",
    "\n",
    "    # Step 1: Vectorized sampling by non-majority selection (fast, will get approximately the correct amount of new samples per class to balance, but not exactly the same count, because it's probabilistic)\n",
    "    yvalcount = y.value_counts()\n",
    "    # Convert the counts into the remainder frequencies (what is needed to fill in each class to get equal with the majority). The majority class will get 0 by definition (ie, it will never be sampled).\n",
    "    yfreq = (yvalcount.max() / yvalcount) - 1.0\n",
    "    #yfreq = (yvalcount.max() - yvalcount) / yvalcount.max()\n",
    "    # Normalize frequencies to 1 (to have probabilities)\n",
    "    yfreq = yfreq / yfreq.sum()\n",
    "    # Calculate the number of samples missing for all classes to be exactly equal\n",
    "    yntogen = (yvalcount.max() * len(yvalcount)) - yvalcount.sum()\n",
    "    # Generate a weight vector, where each index is a sample and the weight is the one corresponding to the yfreq for this class\n",
    "    yfreq2 = y.replace(yfreq)\n",
    "    # Sample!\n",
    "    yidx = y.sample(n=yntogen, replace=True, weights=yfreq2, random_state=0)\n",
    "    # Get the new samples in X and y\n",
    "    Xnew = X.loc[yidx.index, :]\n",
    "    ynew = y.loc[yidx.index]\n",
    "    # Append the new samples to the original dataframes/series\n",
    "    X2 = pd.concat([X, Xnew], ignore_index=True)\n",
    "    y2 = pd.concat([y, ynew], ignore_index=True)\n",
    "\n",
    "    # Step 2: Non-vectorized sampling by non-majority selection (slower because non vectorized, but allows to match exactly the same count for all classes)\n",
    "    y2count = y2.value_counts()\n",
    "    i = 0\n",
    "    while (y2count != y2count.max()).any():  # loop until all classes have the same number of samples\n",
    "        i += 1\n",
    "        # Get any sample that is not of the majority class\n",
    "        yrow = y2[~y2.isin(y2.mode(dropna=False))].sample(n=1, replace=True, random_state=i)\n",
    "        # Duplicate it\n",
    "        X2 = X2.append(X2.loc[yrow.index, :], ignore_index=True)\n",
    "        y2 = y2.append(y2.loc[yrow.index], ignore_index=True)\n",
    "        # Update the count\n",
    "        y2count = y2.value_counts()\n",
    "\n",
    "    return X2, y2\n",
    "\n",
    "if oversample:\n",
    "    print('Random oversampling for class balancing done!')\n",
    "    print('Before oversampling:')\n",
    "    print(y_train.value_counts())\n",
    "    X_train, y_train = random_oversample(X_train, y_train)\n",
    "    print('\\n')\n",
    "    print('After oversampling')\n",
    "    print(y_train.value_counts())\n",
    "    # Ensure the oversampling worked consistently for both X and y (should have the same number of samples in both)\n",
    "    assert(len(X_train) == len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the last element was correctly duplicated \n",
    "lastx = '|'.join(str(x) for x in X_train.iloc[-1, :])\n",
    "simx = X_train.apply(lambda x: '|'.join(str(xx) for xx in x), axis=1)\n",
    "print(y_train.loc[simx == lastx])\n",
    "X_train[simx == lastx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DropOut-like oversampling, to duplicate samples but with values randomly nulled, so that the machine learning algo will hav to learn to predict with less features, and thus be more robust\n",
    "dropout = True\n",
    "n = 0.1 # percentage of the dataset to generate with randomly nullified features\n",
    "\n",
    "#if dropout:\n",
    "def dropout_oversampling(X, y, n=0.1, proba=0.1, nullarr=None, exclude=None):\n",
    "    '''DropOut-like oversampling, to duplicate samples but with values randomly nulled, so that the machine learning algo will hav to learn to predict with less features, and thus be more robust\n",
    "    Note: the sampling is uniform. Balance your classes before using a random oversampling if you want to balance and ensure all classes will have some nullified features, else the uniform distribution should more or less maintain the same class (im)balance without any impact. The goal of this procedure is not to balance out classes, but to ensure the machine learning can also work and have good performances with null features.\n",
    "    X and y need to have the same indices\n",
    "    n is the number of samples to add, it can be a float to be a ration compared to the length of X/y.\n",
    "    proba is the probability of nulling PER feature/column, not for a whole row. If it > 1, then this will represent the amount of columns that should be nullified for sure (the choice will be random), eg: n=2 to nullify 2 columns of each sample.\n",
    "    nullarr is a dictionary of all X's columns names as keys and the null values appropriate for each feature/column as values. If set to None, will try to autodetect based on column's dtype.\n",
    "    exclude is the list of columns in X to exclude from nullifying.\n",
    "    '''\n",
    "    # For reproducibility\n",
    "    #np.random.seed(0)\n",
    "\n",
    "    # If n is a ratio, convert to an integer number of samples\n",
    "    if n < 1.0:\n",
    "        n = int(n * len(y))\n",
    "\n",
    "    # Autodetect appropriate null values for each column's dtype\n",
    "    if nullarr is None:\n",
    "        nullarr = {}\n",
    "        for col in X.columns:\n",
    "            if X[col].dtype.kind == 'O':\n",
    "                # Object or string\n",
    "                nullarr[col] = None\n",
    "            elif X[col].dtype.kind == 'f':\n",
    "                # Float\n",
    "                nullarr[col] = float('NaN')\n",
    "            elif X[col].dtype.kind == 'i':\n",
    "                # Integer\n",
    "                if (X[col] == None).any():\n",
    "                    # New Integer64 type supports None values, if we find any in the column, then we go for it\n",
    "                    nullarr[col] = None\n",
    "                else:\n",
    "                    # Else we fall back to 0 (but might be wrong!)\n",
    "                    nullarr[col] = 0\n",
    "            else:\n",
    "                raise(ValueError('Could not autodetect an appropriate null value for column %s dtype %s, please provide your own nullarr.' % (col, str(X[col].dtype))))\n",
    "\n",
    "    # Calculate all probabilities of all features/columns and samples at once, this is faster (but if dataset is too big it can cause memory issues)\n",
    "    #P = np.random.uniform(size=(n, X.shape[1]))\n",
    "    for i in range(n):\n",
    "        # Calculate the probability to null for all columns of next row\n",
    "        P = np.random.uniform(size=(1, X.shape[1]))\n",
    "        \n",
    "        # Get the probability threshold\n",
    "        if proba >= 1:\n",
    "            # If it's an integer bigger than one, this is the number of columns to nullify, get the corresponding probability\n",
    "            probathresh = np.sort(P)[0][int(proba-1)]\n",
    "        else:\n",
    "            # Else it's directly a probability threshold\n",
    "            probathresh = proba\n",
    "\n",
    "        # Select a sample randomly\n",
    "        yidx = y.sample(n=1, random_state=i)\n",
    "        # Get the sample\n",
    "        Xnew = X.loc[yidx.index, :]\n",
    "        ynew = y.loc[yidx.index]\n",
    "        # Loop through each column\n",
    "        for (col, _), p, nullval in zip(Xnew.items(), P[0], nullarr):\n",
    "            # Skip excluded columns\n",
    "            if exclude is not None and col in exclude:\n",
    "                continue\n",
    "            # Nullify if the probability is below the threshold\n",
    "            elif p <= probathresh:\n",
    "                Xnew[col] = nullarr[col]\n",
    "\n",
    "        # Append the new dropout-modified sample\n",
    "        X = X.append(Xnew, ignore_index=True)\n",
    "        y = y.append(ynew, ignore_index=True)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "if dropout:\n",
    "    X_train, y_train = dropout_oversampling(X_train, y_train, n, 0.2)\n",
    "    X_train = fillnaseparateclass(X_train) # encode None in object columns as strings, else it won't work for most machine learning packages\n",
    "    assert len(X_train) == len(y_train)\n",
    "    print('Dropout oversampling done!')\n",
    "    print('New size:')\n",
    "    print(len(X_train))\n",
    "    print('Last 20 entries:')\n",
    "    print(X_train.iloc[-20:, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import imblearn\n",
    "    # TODO: both encoded and non encoded datasets won't have the same resampling!\n",
    "\n",
    "balanceclasses = True\n",
    "if balanceclasses:\n",
    "    # SMOTE-NC would be the best, because it interpolates numeric values and thus create new entries, but it does not yet support missing values, and using a placeholder imputation is incorrect: https://github.com/scikit-learn-contrib/imbalanced-learn/issues/157\n",
    "    # Thus we instead use a RandomOverSampler, which will just duplicates as-is the samples to balance the power of each class, without interpolating new realistic samples\n",
    "    oversampler = imblearn.over_sampling.RandomOverSampler(sampling_strategy='not majority', random_state=0) # if smote-nc, use kneighbors = 1 to avoid a majority vote (but then no interpolation!!! Should fix in code to avoid majority vote and just choose randomly, we don't want to do any data cleaning, we can do that afterward!)\n",
    "    # Create a copy\n",
    "    X_trainov = X_train.copy()\n",
    "    X_train_encov = X_train_enc.copy()\n",
    "    # Imputation of nan values to a fake value\n",
    "    X_trainov[X_trainov.isnull()] = -999999999.999999\n",
    "    X_train_encov[X_train_encov.isnull()] = -999999999.999999\n",
    "    # Resample\n",
    "    X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
    "    X_train_enc, y_train_enc = oversampler.fit_resample(X_train_enc, y_train_enc)\n",
    "    # Place back the null values exactly as they were (same type etc)\n",
    "    X_trainov[X_train.isnull()] = X_train\n",
    "    X_train_encov[X_train_enc.isnull()] = X_train_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance in training dataset\n",
    "y_train.value_counts().apply(lambda x: x / len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check class balance in test dataset\n",
    "y_test.value_counts().apply(lambda x: x / len(y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data encoding (fitting and transform, after oversampling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learn encoding of categorical variables with LabelEncoder. It's not optimal but we hope the machine learning algo won't use any ordering artificially created by the label encoder.\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# Encode the target y classes\n",
    "enc_y = preprocessing.LabelEncoder()\n",
    "enc_y.fit(pd.concat([y_orig, y_train]))\n",
    "\n",
    "# Encode the categorical input features in X\n",
    "enc_X = {}\n",
    "enc_X_mapping = {}\n",
    "X_full = pd.concat([X_orig, X_train])  # we add the training dataset because it might contain new values after oversampling (either because of interpolation if using SMOTE-NC, or because of new nullified values if the column was complete before)\n",
    "X_full = fillnaseparateclass(X_full)  # make sure all nans are string, else the label encoder will choke\n",
    "for col in categorical_columns:\n",
    "    enc_x = preprocessing.LabelEncoder()\n",
    "    print('Fitting to column: %s' % col)\n",
    "    enc_x.fit(X_full[col])\n",
    "    enc_X[col] = enc_x\n",
    "    enc_X_mapping[col] = dict(zip((int(x) for x in enc_x.transform(enc_x.classes_)), enc_x.classes_))  # get the mapping, useful for later\n",
    "    print(enc_X_mapping[col])\n",
    "    print('\\n')\n",
    "del X_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([X_orig[col], X_train[col]])\n",
    "X_orig[col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode now\n",
    "y_train_enc = enc_y.transform(y_train)\n",
    "y_test_enc = enc_y.transform(y_test)\n",
    "\n",
    "X_train_enc = fillnaseparateclass(X_train.copy())  # labelencoder unfortunately does not support None values, hence need to encode them as strings...\n",
    "X_test_enc = fillnaseparateclass(X_test.copy())\n",
    "for col in categorical_columns:\n",
    "    X_train_enc[col] = enc_X[col].transform(X_train_enc[col])\n",
    "    X_test_enc[col] = enc_X[col].transform(X_test_enc[col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "mod = xgb.XGBClassifier(\n",
    "    gamma=1,                 \n",
    "    learning_rate=0.01,\n",
    "    max_depth=3,\n",
    "    n_estimators=10000,                                                                    \n",
    "    subsample=0.8,\n",
    "    random_state=34\n",
    ")\n",
    "\n",
    "mod.fit(X_train, y_train)\n",
    "predictions = mod.predict(X_test)\n",
    "rmse = sqrt(mean_squared_error(y_test, predictions))\n",
    "print(\"score: {0:,.0f}\".format(rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CatBoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/vchulski/dota-2-catboost-and-shap-explainer\n",
    "#https://github.com/catboost/tutorials/blob/master/python_tutorial.ipynb\n",
    "import catboost\n",
    "model = catboost.CatBoostClassifier(custom_loss=['Accuracy'], random_seed=0, eval_metric='AUC', logging_level='Silent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['unified.diagnosis_worst']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit CatBoost\n",
    "pyver = 3\n",
    "if pyver == 2:\n",
    "    # For Python 2, CatBoost has currently a bug that makes it incapable of handling categorical features automatically, thus one needs to encode them beforehand\n",
    "    # See https://github.com/catboost/catboost/issues/958\n",
    "    model.fit(X_train_enc, y_train_enc,\n",
    "        cat_features=categorical_columns,\n",
    "        eval_set=(X_test_enc, y_test_enc),\n",
    "        #logging_level='Verbose',  # you can uncomment this for text output\n",
    "        plot=True #Uncomment and you'll see really great real time interactive graph\n",
    "    )\n",
    "else:\n",
    "    # Python 3, CatBoost can handle categorical features automatically, no need for an encoder\n",
    "    model.fit(fillnaseparateclass(X_train), y_train,\n",
    "        cat_features=categorical_columns,\n",
    "        eval_set=(fillnaseparateclass(X_test), y_test),\n",
    "        #logging_level='Verbose',  # you can uncomment this for text output\n",
    "        plot=True #Uncomment and you'll see really great real time interactive graph\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "if pyver == 2:\n",
    "    print('Accuracy on Training set: %f, on Test set: %f' % (accuracy_score(y_train_enc, model.predict(X_train_enc)), accuracy_score(y_test_enc, model.predict(X_test_enc))))\n",
    "else:\n",
    "    print('Accuracy on Training set: %f, on Test set: %f' % (accuracy_score(y_train, model.predict(X_train)), accuracy_score(y_test, model.predict(X_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csg_fileutil_libs.model_evaluation_utils import model_evaluation_utils as meu\n",
    "\n",
    "# Per class accuracy\n",
    "# using library from https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch05_Building_Tuning_and_Deploying_Models/model_evaluation_utils.py as explained in https://towardsdatascience.com/explainable-artificial-intelligence-part-3-hands-on-machine-learning-model-interpretation-e8ebe5afc608\n",
    "meu.display_model_performance_metrics(true_labels=y_train,\n",
    "                                      predicted_labels=model.predict(X_train),\n",
    "                                      classes=y_orig.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meu.display_model_performance_metrics(true_labels=y_test,\n",
    "                                      predicted_labels=model.predict(X_test),\n",
    "                                      classes=y_orig.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.plot_tree(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with missing values where it was complete, to check if the classifier can still work\n",
    "onetest = pd.DataFrame([['vs/uws', 1.0, 10, float('NaN'), 'M', True, '']], columns=X_train.columns)\n",
    "model.predict(onetest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: save the model to be able to reload it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MICE imputer: the best approach but sklearn does NOT support categorical variables, so we would need to first use a labelencoder, but then the MICE imputer will infer an ordering in categorical variables where there is none\n",
    "\n",
    "# explicitly require this experimental feature\n",
    "#from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "# now you can import normally from sklearn.impute\n",
    "#from sklearn.impute import IterativeImputer\n",
    "\n",
    "#imp = IterativeImputer(sample_posterior=True, random_state=0)\n",
    "#X_train_imp = imp.fit_transform(X_train)\n",
    "#X_train_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple custom imputer: replace floats nan values with the median\n",
    "# This is necessary for Skater to work\n",
    "X_train_imp = X_train.copy()\n",
    "X_test_imp = X_test.copy()\n",
    "X_train_encimp = X_train_enc.copy()\n",
    "X_test_encimp = X_test_enc.copy()\n",
    "for col in X_orig.columns:\n",
    "    if X_orig[col].dtype == 'float':\n",
    "        for x_imp in (X_train_imp, X_test_imp, X_train_encimp, X_test_encimp):\n",
    "            x_imp.loc[x_imp[col].isnull(), col] = x_imp[col].median()\n",
    "print('Imputing done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skater"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skater import about\n",
    "from packaging import version\n",
    "skaterver = about.__version__\n",
    "if version.parse(skaterver) < version.parse('1.1.2'):\n",
    "    raise ImportError('Need Skater at least version 1.1.2 to use it here, current version installed is: %s. Use the conda build here: https://anaconda.org/derickl/skater' % skaterver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skater.core.explanations import Interpretation\n",
    "from skater.model import InMemoryModel\n",
    "\n",
    "interpreter = Interpretation(X_train_imp, feature_names=list(X_orig.columns))\n",
    "\n",
    "# If you get an OverflowError when using plot in the cell after, it's probably because you have nan values in your floats, see for a fix: https://github.com/oracle/Skater/issues/285\n",
    "#interpreter.load_data(X_train, feature_names=list(X_orig.columns))\n",
    "im_model = InMemoryModel(model.predict_proba,\n",
    "                         examples=X_test_imp,\n",
    "                         target_names=y_train.unique(),\n",
    "                         #unique_values=y_train.unique(),\n",
    "                         #model_type='classifier',\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot barplot of features importance\n",
    "plots = interpreter.feature_importance.plot_feature_importance(im_model, ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plots2 = interpreter.partial_dependence.plot_partial_dependence(list(X_orig.columns), im_model, with_variance=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://oracle.github.io/Skater/reference/interpretation.html#interpretable-rule-based\n",
    "# Need to install skater >= 1.1.2, via this conda build: https://anaconda.org/derickl/skater\n",
    "# Works only on label encoded categorical variables and without nans\n",
    "\n",
    "from skater.util.dataops import convert_dataframe_to_dict, show_in_notebook\n",
    "\n",
    "surrogate_explainer = interpreter.tree_surrogate(im_model, seed=0)\n",
    "surrogate_explainer.fit(X_train_encimp, y_train, use_oracle=True, prune='post', scorer_type='default')\n",
    "surrogate_explainer.plot_global_decisions(colors=['coral', 'lightsteelblue','darkkhaki'],\n",
    "                                          file_name='simple_tree_pre.png')\n",
    "show_in_notebook('simple_tree_pre.png', width=400, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print all the predicted diagnoses, with and without encoding, if some are missing then there is a problem\n",
    "print(np.unique(model.predict(X_train)))\n",
    "print(np.unique(model.predict(X_train_encimp)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: learn my own decision tree with probabilities of decision and that supports categorical variables and empty values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from skater.core.global_interpretation.interpretable_models.brlc import BRLC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features importance: SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "if version.parse(skimage.__version__) < version.parse('0.14.2'):\n",
    "    raise ImportError('scikit-image > v0.14.2 is required, but v%s is installed, please update (use `conda install -c conda-forge scikit-image`).' % skimage.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[X_train.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import catboost\n",
    "# Workaround because shap does not support categorical features yet\n",
    "shap_values = model.get_feature_importance(catboost.Pool(X_train, y_train, cat_features=categorical_columns), type=catboost.EFstrType.ShapValues)\n",
    "\n",
    "expected_value = shap_values[0,-1]\n",
    "shap_values2 = shap_values[:,:-1]\n",
    "\n",
    "shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2 = X_train.copy()\n",
    "for col in X_train2.columns:\n",
    "    if X_train2[col].dtype == 'str' or X_train2[col].dtype == 'object':\n",
    "        X_train2.loc[X_train2[col].isnull(), col] = 'None'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explainer = shap.TreeExplainer(model)\n",
    "#shap_values = explainer.shap_values(X_train2, approximate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install latest version of SHAP else it won't work (not for TreeExplainer at least, but KernelExplainer should work)\n",
    "# See for workarounds https://github.com/slundberg/shap/issues/662\n",
    "# and the fix https://github.com/slundberg/shap/issues/736\n",
    "explainer = shap.KernelExplainer(model.predict_proba, X_train)\n",
    "#shap_values = explainer.shap_values(X_train)\n",
    "shap_values = model.get_feature_importance(catboost.Pool(X_train, y_train, cat_features=categorical_columns), type='ShapValues')\n",
    "print(shap_values.shape)\n",
    "\n",
    "# visualize the first prediction's explanation\n",
    "# use link logit to visualize probabilities instead of shap values. This is yet unavailable for summary plot: https://github.com/slundberg/shap/issues/756\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], feature_names=X_orig.columns, out_names=list(y_train.unique()), link='logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All features pairwise shap values interactions\n",
    "# THIS IS WRONG! it's not the pairwise shap values interactions!\n",
    "interaction_values = model.get_feature_importance(catboost.Pool(X_train, y_train, cat_features=categorical_columns), type='Interaction')\n",
    "print(interaction_values.shape)\n",
    "print(interaction_values)\n",
    "shap.summary_plot(shap_values[:,:-1,:-1], features=X_train, plot_type='dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the effects of all the features in a multi-class barplot\n",
    "# See for more explanations on how it was done: https://github.com/slundberg/shap/issues/750\n",
    "# To understand how to interpret this graphic, see: https://github.com/slundberg/shap/issues/367\n",
    "# TODO: fix order of class names, not sure this is the order used by catboost!\n",
    "\n",
    "# Transpose the 3D numpy ndarray (shape [#samples, #classes, #features]) so we place the classes first\n",
    "#original_shape = shap_values.shape\n",
    "#shap_values_reshaped = shap_values.reshape(original_shape[1], original_shape[0], original_shape[-1], order='C') # not the correct way of doing it, you will get the same mean SHAP value for all classes!\n",
    "shap_values_transposed = shap_values.transpose(1, 0, 2)\n",
    "assert shap_values[0, 1, 2] == shap_values_transposed[1, 0, 2]  # just check we've done it right\n",
    "print(shap_values_transposed.shape)\n",
    "\n",
    "# Then we convert the transposed shap values into a list, where each element will be a 2D numpy matrix of shape [#samples, #features] as expected by the shap package\n",
    "shap.summary_plot(list(shap_values_transposed[:,:,:-1]), features=X_train, class_names=y_train.unique(), plot_type='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features for each class output\n",
    "for yclass in range(len(shap_values_transposed)):\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values_transposed[yclass,:,:-1], features=X_train_enc, plot_type='violin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the effects of all the features for each class output\n",
    "for yclass in range(len(shap_values_transposed)):\n",
    "    plt.figure()\n",
    "    shap.summary_plot(shap_values_transposed[yclass,:,:-1], features=X_train_enc, plot_type='dot') # color_bar_label change to class name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot('unified.diagnosis_worst', shap_values_transposed[0, :, :-1], X_train_enc, display_features=X_train, # display_features allows to specify string labels for the features, it should be the non-encoded dataset, so the same matrix size as for the argument \"features\"\n",
    "                    x_jitter=0.2, interaction_index='unified.etiology') #The index of the feature used to color the plot. The name of a feature can also be passed as a string. If “auto” then shap.common.approximate_interactions is used to pick what seems to be the strongest interaction (note that to find to true stongest interaction you need to compute the SHAP interaction values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELI5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: https://eli5.readthedocs.io/en/latest/_modules/eli5/catboost.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use permutation of ELI5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BESTree BESTForest classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
