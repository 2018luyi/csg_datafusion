{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIfTI modular reorganizer (aka NeuroDataset Builder)\n",
    "By Stephen Larroque @ Coma Science Group, GIGA Research, University of Liege\n",
    "Creation date: 2019-03-21\n",
    "License: MIT\n",
    "v1.0.2\n",
    "\n",
    "DESCRIPTION:\n",
    "This tool allows to automatically organize (copy) NIfTI folders (one per subject/session) into neatly organized folders according to a supplied demographics file.\n",
    "\n",
    "The goal of this modular reorganizer is to allow updating the demographics and/or adding more niftis without having to redo all the conversion process, which is both quite time consuming and requires to reorient again all structural and functions! Here you reorient/coregister once all subjects, and THEN you build your dataset given your demographics csv file and selection/filtering criteria of choice.\n",
    "\n",
    "\n",
    "\n",
    "INSTALL NOTE:\n",
    "You need to pip install pandas before launching this script.\n",
    "Tested on Python 2.7.15\n",
    "You need also mcverter, as part of [MRIConvert](https://lcni.uoregon.edu/downloads/mriconvert).\n",
    "\n",
    "USAGE:\n",
    "Input:\n",
    "* the final unified and postprocessed database (merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall.csv), resulting from using [csg_datafusion_finaldbunification.ipynb](csg_datafusion_finaldbunification.ipynb), or another csv file containing same info as can be found on nifti folders naming (eg, subject name and study date, like 'bernard-dupont_2019-01-01' as nifti folder name, and in database you have two rows 'name' with \"Bernard Dupont\" and 'StudyDate' with \"01/01/2019\" - everything can be configured in the parameters below).\n",
    "* a rootpath folder where each folder = one subject/folder, with the folders being named according to the demographics file (use [csg_datafusion_dicoms_to_nifti.ipynb](csg_datafusion_dicoms_to_nifti.ipynb) or from dicom infos (use dcm2niix with specific formatting to save the subject id/name and study date in folder name).\n",
    "\n",
    "TODO:\n",
    "* Nothing here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcefully autoreload all python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX FUNCTIONS\n",
    "\n",
    "import os, sys\n",
    "\n",
    "cur_path = os.path.realpath('.')\n",
    "sys.path.append(os.path.join(cur_path, 'csg_fileutil_libs'))  # for unidecode and cleanup_name, because it does not support relative paths (yet?)\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "from csg_fileutil_libs.aux_funcs import save_df_as_csv, _tqdm, reorder_cols_df, find_columns_matching, cleanup_name, df_to_unicode, df_to_unicode_fast, cleanup_name_df, df_literal_eval, reorder_cols_df, create_dir_if_not_exist, copy_any, get_list_of_folders, merge_two_df\n",
    "from IPython.display import display\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# Unified post-processed demographics database\n",
    "unified_csv = r'databases_output\\merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall_nifti_mov.csv'\n",
    "# Input folder where all nifti folders are located (one folder per subject/session, they must ALL be stored at the same level, side by side)\n",
    "input_dir = r'G:\\hyperc_doc\\niftis2'\n",
    "# Output folder for converted NIFTI files (a subfolder for each key will be created)\n",
    "output_dir = r'G:\\hyperc_doc\\nhdoc4'\n",
    "\n",
    "# Mode selector\n",
    "# mode can be 'demographics' or 'niftis':\n",
    "# * if 'demographics', will use the demographics to build the path to the niftis (will derive the folder name based on some key_columns);\n",
    "# * if 'niftis', will extract infos from the niftis folders based on the specified template, and will then compare/merge against the specified key_columns in the demographics database.\n",
    "# In other words: 'demographics' use the demographics to find the niftis, whereas 'niftis' starts from the niftis and then try to find the corresponding demographics entries. Use 'demographics' if you followed the whole csg_datafusion pipeline, else if you converted from dicoms to niftis by yourself (eg, using dcm2niix without csg_datafusion), then use 'niftis'.\n",
    "script_mode = 'demographics'\n",
    "\n",
    "# -- DEMOGRAPHICS MODE\n",
    "# Columns in the demographics that were used to generate the nifti folders names from demographics (use the same here as in dicoms_to_nifti.ipynb)\n",
    "# Note 1: rows will be filtered if any of these key columns is empty\n",
    "# Note 2: the resulting rows need to be unique: not any two rows should have the same key columns (all combined)\n",
    "# Note 3: later for automatic dataset reorganization, you will need to input the same key columns\n",
    "key_columns = ['name', 'StudyDate']  # only for script_mode == 'demographics'\n",
    "\n",
    "# -- NIFTIS MODE\n",
    "# Naming template for niftis folders, to extract the pertinent variables. The regex group names should be the same as the key_columns_merge used to merge these infos with the demographics (so it should be the names of columns in the demographics csv file).\n",
    "folder_template = r'(?P<name>[^_]+)_(?P<StudyDate>[^_]+)'  # only for script_mode == 'niftis'\n",
    "# Please indicate here the name of the columns and their type for the merge with the demographics csv file\n",
    "key_columns_merge = OrderedDict([('name', 'id'), ('StudyDate', 'datetime|%Y-%m-%d')])  # only for script_mode == 'niftis'\n",
    "# Save list of nifti folders as a csv file?\n",
    "save_nifti_list = True\n",
    "\n",
    "# -- FOR BOTH MODES\n",
    "# Filter function\n",
    "# define here what rows will be selected for the reorganization. This will filter out all the rows you don't want to keep.\n",
    "# in other words, this is where you define which subjects/sessions you select for your study.\n",
    "# this should return the dataframe filtered by any condition you want (make sure to return at least the key_columns and hierarchy_cols for the rest of the script to work)\n",
    "def my_filter_func(cf_unified):\n",
    "    return cf_unified[\n",
    "                        #(cf_unified['unified.diagnosis_best'] == 'emcs') \\\n",
    "                        #(cf_unified['unified.diagnoses_count'] >= 3) \\\n",
    "                      (cf_unified['nifti.struct OK (for fmri)'].isin(['O','M'])) \\\n",
    "                      & (cf_unified['nifti.func OK'].isin(['O','M'])) \\\n",
    "                      & (cf_unified['unified.episedationsimple'].isin(['no','both']))\n",
    "                     ]\n",
    "filter_func = my_filter_func\n",
    "\n",
    "# Hierarchy columns\n",
    "# define here what hierarchies should be used to create the subdirectory trees, in the order of the list (ie, 1st column's values will be top parent, then 2nd column's values is subdirectory, then 3rd column is subsubdirectory, etc)\n",
    "#hierarchy_cols = ['unified.episedationsimple', 'unified.etiology']\n",
    "hierarchy_cols = ['unified.diagnosis_best']\n",
    "# prepend column name before the value in the folder name (be careful that the output filepath does not get too long, or you might run into errors!)\n",
    "hierarchy_prepend_colname = False\n",
    "# In case the value for a field is missing (for the hierarchy columns), what should we replace it with?\n",
    "placeholder_value = 'unknown'\n",
    "\n",
    "# Skip conversion errors?\n",
    "skip_errors = True\n",
    "# Cleanup names to replace accentuated and special characters? (advised, please use same setting as in dicoms_to_nifti.ipynb)\n",
    "clean_names = True\n",
    "\n",
    "# Special parameters\n",
    "verbose = False\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv db as dataframe\n",
    "import pandas as pd\n",
    "\n",
    "cf_unified = pd.read_csv(unified_csv, sep=';', low_memory=False).dropna(axis=0, how='all').fillna('')  # drop empty lines\n",
    "cf_unified = df_to_unicode_fast(cf_unified, progress_bar=True)  # convert to unicode (can fix issues with accentuated characters)\n",
    "cf_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subset with non empty key columns and dicom column (ie, dicom is available)\n",
    "if script_mode == 'niftis':\n",
    "    key_columns = key_columns_merge.keys()\n",
    "cf_unified_nonempty = cf_unified[~(cf_unified[key_columns].isnull() | (cf_unified[key_columns] == '')).any(axis=1)]\n",
    "cf_unified_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an id for each subject/session (will be used as the output folder name)\n",
    "# from dicoms_to_nifti.ipynb\n",
    "# TODO: make a function in aux.py to be shared\n",
    "# TODO: useless in script_mode niftis?\n",
    "def df_concat_cols(x):\n",
    "    \"\"\"Concatenate values over different columns\"\"\" \n",
    "    return '_'.join(x).strip().replace(' ','-')\n",
    "\n",
    "idcol = df_concat_cols(key_columns)\n",
    "cf_unified_nonempty.loc[:, idcol] = cf_unified_nonempty.loc[:, key_columns].apply(df_concat_cols, axis=1)\n",
    "if clean_names:\n",
    "    cf_unified_nonempty.loc[:, idcol] = cf_unified_nonempty.loc[:, idcol].apply(cleanup_name).apply(lambda x: x.replace(' ', '_'))\n",
    "cf_clean = cf_unified_nonempty[~cf_unified_nonempty[idcol].isnull()]\n",
    "cf_clean[idcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to keep only the rows we are interested in\n",
    "cf_filtered = filter_func(cf_clean)\n",
    "cf_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract metadata from nifti folders names (using the provided regex)\n",
    "if script_mode == 'niftis' or save_nifti_list:\n",
    "    key_columns = key_columns_merge.keys()\n",
    "    RE_folder = re.compile(folder_template, re.I)  # precompile the regex to speed up calculations\n",
    "    niftis_metadata = []\n",
    "    # For each nifti folder\n",
    "    for p in get_list_of_folders(input_dir):\n",
    "        try:\n",
    "            # Create a list of dictionaries\n",
    "            niftis_metadata.append({})\n",
    "            # Add the nifti folder\n",
    "            niftis_metadata[-1]['niftifolder'] = p\n",
    "            # And add each specified metadata (with the name of the regex named group being the column/metadata name)\n",
    "            for k in key_columns:\n",
    "                niftis_metadata[-1][k] = RE_folder.search(p).group(k)\n",
    "        except Exception as exc:\n",
    "            print('ERROR: the provided regex template (in folder_template) does not match some nifti folders names! Please check your regex template and retry!')\n",
    "            print('Choked on nifti folder: %s' % p)\n",
    "            raise exc\n",
    "\n",
    "    # Convert a DataFrame\n",
    "    cf_niftis = pd.DataFrame(niftis_metadata)\n",
    "    # Save list of nifti in a csv file\n",
    "    if save_nifti_list:\n",
    "        cf_niftis_unicode = df_to_unicode_fast(cf_niftis)\n",
    "        if save_df_as_csv(cf_niftis_unicode, unified_csv[:-4]+'_niftis.csv', fields_order=False, date_format='%Y-%m-%d'):\n",
    "            print('Input niftis list saved in %s!' % (unified_csv[:-4]+'_niftis.csv'))\n",
    "        else:\n",
    "            print('ERROR: the input niftis list could not be saved!')\n",
    "    # Display all input niftis\n",
    "    display(cf_niftis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the demographics and niftis databases based on the key columns (usually name and StudyDate)\n",
    "if script_mode == 'niftis':\n",
    "    cf_merge_mapping, cf_merge = merge_two_df(cf_filtered, cf_niftis, col=[key_columns_merge, key_columns_merge],\n",
    "                                  skip_sanity=True, keep_nulls=False,\n",
    "                                  returnmerged=True,\n",
    "                                  join_on_shared_keys=False)\n",
    "    print('Done!')\n",
    "    display(cf_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN LOOP\n",
    "# Copying the nifti folders with the generated hierarchy\n",
    "if script_mode == 'demographics':\n",
    "    cf_loop = cf_filtered\n",
    "elif script_mode == 'niftis':\n",
    "    cf_loop = cf_merge\n",
    "else:\n",
    "    raise Exception('ERROR: script_mode %s is undefined! Cannot continue!' % scrip_mode)\n",
    "\n",
    "conflicts = []\n",
    "missing = []\n",
    "# For each row\n",
    "for idx, row in _tqdm(cf_loop.iterrows(), total=len(cf_loop), desc='REORG', unit='sessions'):\n",
    "    # Build the input and output paths\n",
    "    if script_mode == 'demographics':\n",
    "        foldername = row[idcol]\n",
    "    elif script_mode == 'niftis':\n",
    "        foldername = row['niftifolder']\n",
    "    input_filepath = os.path.join(input_dir, row[idcol])\n",
    "\n",
    "    if not os.path.exists(input_filepath):\n",
    "        # Missing input file, we skip!\n",
    "        missing.append(input_filepath)\n",
    "    else:\n",
    "        # Organize per the specified hierarchy\n",
    "        outpath = []\n",
    "        for hcol in hierarchy_cols:  # select the columns to use as hierarchy\n",
    "            try:\n",
    "                # Get the value for this column\n",
    "                v = row[hcol]\n",
    "                # If empty, raise an error\n",
    "                if not v.strip():\n",
    "                    raise Exception('empty value')\n",
    "            except Exception as exc:\n",
    "                # If error (value empty or inexistent), we use a placeholder value\n",
    "                if verbose:\n",
    "                    print('Warning: no or empty value for hierarchical column %s for row id %s' % (hcol, row[idcol]))\n",
    "                v = placeholder_value\n",
    "            # Prepend the column name if option enabled\n",
    "            if hierarchy_prepend_colname:\n",
    "                v = '%s_%s' % (hcol, v)\n",
    "            # Add the value to the list of subfolders\n",
    "            outpath.append(v)\n",
    "        # Append the subject name as the final subfolder\n",
    "        outpath.append(foldername)\n",
    "        # Build the final path, prepending the output directory\n",
    "        output_filepath = os.path.join(output_dir, *outpath)\n",
    "        # Check if there is a conflict (output already exists)\n",
    "        if os.path.exists(output_filepath):\n",
    "            conflicts.append([input_filepath, output_filepath])\n",
    "        # Copy recursively!\n",
    "        try:\n",
    "            copy_any(input_filepath, output_filepath)\n",
    "        except Exception as exc:\n",
    "            print('Error when copying: maybe the constructed path is too long for your OS? Then please revise your parameters (reduce hierarchy for example). Full error:')\n",
    "            print(exc)\n",
    "        # Debug stuff\n",
    "        if debug:\n",
    "            break\n",
    "\n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "if missing:\n",
    "    with open('niftis_missing.txt', 'w') as f:\n",
    "        f.write(pprint.pformat(missing, indent=4, width=80))\n",
    "    print('\\nSome nifti folders were not found, the list is saved in niftis_unprocessed.txt')\n",
    "else:\n",
    "    print('\\nAll nifti folders were processed!')\n",
    "if conflicts:\n",
    "    with open('niftis_conflicts.txt', 'w') as f:\n",
    "        f.write(pprint.pformat(conflicts, indent=4, width=80))\n",
    "    print('\\nSome nifti folders were in conflicts and got overwritten, the list is saved in niftis_conflicts.txt')\n",
    "else:\n",
    "    print('\\nNo conflicts found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the subset of selected entries into demographics csv files\n",
    "cf_loop_unicode = df_to_unicode_fast(cf_loop)\n",
    "cf_loop_extended = cf_unified[cf_unified[key_columns[0]].isin(cf_loop[key_columns[0]])]\n",
    "cf_loop_extended_unicode = df_to_unicode_fast(cf_loop_extended)\n",
    "if save_df_as_csv(cf_loop_unicode, unified_csv[:-4]+'_reorganizedsubset.csv', fields_order=False, date_format='%Y-%m-%d'):\n",
    "    save_df_as_csv(cf_loop_extended_unicode, unified_csv[:-4]+'_reorganizedsubsetextended.csv', fields_order=False, date_format='%Y-%m-%d')\n",
    "    print('Subset demographics for the reorganized database successfully saved in %s and %s!' % (unified_csv[:-4]+'_reorganizedsubset.csv', unified_csv[:-4]+'_reorganizedsubsetextended.csv'))\n",
    "else:\n",
    "    print('ERROR: the subset demographics for the reorganized database could not be saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extended subset of selected entries into demographics csv files\n",
    "# by first disambiguate any similar name, and then save any entry that can be relevant\n",
    "# this way, we make sure we don't miss any oddly named (eg, typo in name) entries\n",
    "\n",
    "# Disambiguate names\n",
    "cf_filtered_extended2_mapping = merge_two_df(cf_unified, cf_unified, col=key_columns[0], returnmerged=False, skip_sanity=True)\n",
    "# Filter the name mapping\n",
    "cf_filtered_extended2_mapping2 = cf_filtered_extended2_mapping.loc[cf_filtered_extended2_mapping[key_columns[0]].isin(cf_filtered[key_columns[0]].unique()), :]\n",
    "# Get all entries from cf_unified that match either of the filtered disambiguated names\n",
    "cf_filtered_extended2 = cf_unified.loc[cf_unified[key_columns[0]].isin(cf_filtered_extended2_mapping2[key_columns[0]]) | cf_unified[key_columns[0]].isin(cf_filtered_extended2_mapping2[key_columns[0]+'2']), :]\n",
    "# Save extended infos about these entries (and convert to unicode first)\n",
    "cf_filtered_extended2_unicode = df_to_unicode_fast(cf_filtered_extended2)\n",
    "if save_df_as_csv(cf_filtered_extended2_unicode, unified_csv[:-4]+'_reorganizedsubsetextended2.csv', fields_order=False, csv_order_by=key_columns, date_format='%Y-%m-%d'):\n",
    "    print('Subset demographics for the reorganized database successfully saved in %s!' % (unified_csv[:-4]+'_reorganizedsubsetextended2.csv'))\n",
    "else:\n",
    "    print('ERROR: the extended subset demographics for the reorganized database could not be saved!')\n",
    "# Display the entries\n",
    "cf_filtered_extended2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
