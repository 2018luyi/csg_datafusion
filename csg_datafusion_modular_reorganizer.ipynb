{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NIfTI modular reorganizer (aka NeuroDataset Builder)\n",
    "By Stephen Larroque @ Coma Science Group, GIGA Research, University of Liege\n",
    "Creation date: 2019-03-21\n",
    "License: MIT\n",
    "v0.5.0\n",
    "\n",
    "DESCRIPTION:\n",
    "This tool allows to automatically organize (copy) NIfTI folders (one per subject/session) into neatly organized folders according to a supplied demographics file.\n",
    "\n",
    "\n",
    "\n",
    "INSTALL NOTE:\n",
    "You need to pip install pandas before launching this script.\n",
    "Tested on Python 2.7.15\n",
    "You need also mcverter, as part of [MRIConvert](https://lcni.uoregon.edu/downloads/mriconvert).\n",
    "\n",
    "USAGE:\n",
    "Input:\n",
    "* the final unified and postprocessed database (merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall.csv), resulting from using [csg_datafusion_finaldbunification.ipynb](csg_datafusion_finaldbunification.ipynb)\n",
    "* a rootpath folder where each folder = one subject/folder, with the folders being named according to the demographics file (use [csg_datafusion_dicoms_to_nifti.ipynb](csg_datafusion_dicoms_to_nifti.ipynb) or from dicom infos (use dcm2niix with specific formatting to save the subject id/name and study date in folder name).\n",
    "\n",
    "TODO:\n",
    "* support both optionally (allow for database merge with partial match based on folder name with a custom template to say what part of the folder name should be what in the excel database, or just use names as-is for exact match). In other words: for dicom mode, allow template with variables to catch with regex in the folder name, and then compare in exact order in function (or named regex groups?) and merge (name column and date column with specific cases, use df_merge function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcefully autoreload all python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX FUNCTIONS\n",
    "\n",
    "import os, sys\n",
    "\n",
    "cur_path = os.path.realpath('.')\n",
    "sys.path.append(os.path.join(cur_path, 'csg_fileutil_libs'))  # for unidecode and cleanup_name, because it does not support relative paths (yet?)\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "from collections import OrderedDict\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "from csg_fileutil_libs.aux_funcs import save_df_as_csv, _tqdm, reorder_cols_df, find_columns_matching, cleanup_name, df_to_unicode, df_to_unicode_fast, cleanup_name_df, df_literal_eval, reorder_cols_df, create_dir_if_not_exist, copy_any, get_list_of_folders, merge_two_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# Unified post-processed demographics database\n",
    "unified_csv = r'databases_output\\merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall.csv'\n",
    "# Input folder where all nifti folders are located (one folder per subject/session, they must ALL be stored at the same level, side by side)\n",
    "input_dir = r'C:\\git\\datatest\\output\\niftis'\n",
    "\n",
    "script_mode = 'demographics'  # mode can be 'demographics' or 'niftis': if 'demographics', will use the demographics to build the path to the niftis (will derive the folder name based on some key_columns) ; if 'niftis', will extract infos from the niftis folders based on the specified template, and will then compare/merge against the specified key_columns in the demographics database. In other words: 'demographics' use the demographics to find the niftis, whereas 'niftis' starts from the niftis and then try to find the corresponding demographics entries. Use 'demographics' if you followed the whole csg_datafusion pipeline, else if you converted from dicoms to niftis by yourself (eg, using dcm2niix without csg_datafusion), then use 'niftis'.\n",
    "# Columns in the demographics that were used to generate the nifti folders names from demographics (use the same here as in dicoms_to_nifti.ipynb)\n",
    "# Note 1: rows will be filtered if any of these key columns is empty\n",
    "# Note 2: the resulting rows need to be unique: not any two rows should have the same key columns (all combined)\n",
    "# Note 3: later for automatic dataset reorganization, you will need to input the same key columns\n",
    "key_columns = ['name', 'StudyDate']  # only for script_mode == 'demographics'\n",
    "\n",
    "# Naming template for niftis folders, to extract the pertinent variables. The regex group names should be the same as the key_columns_merge used to merge these infos with the demographics (so it should be the names of columns in the demographics csv file).\n",
    "folder_template = r'(?P<name>[^_]+)_(?P<StudyDate>[^_]+)'  # only for script_mode == 'niftis'\n",
    "# Please indicate here the name of the columns and their type for the merge with the demographics csv file\n",
    "key_columns_merge = OrderedDict([('name', 'id'), ('StudyDate', 'datetime|%Y-%m-%d')])  # only for script_mode == 'niftis'\n",
    "\n",
    "# Filter function\n",
    "# define here what rows will be selected for the reorganization. This will filter out all the rows you don't want to keep.\n",
    "# this should return the dataframe filtered by any condition you want (make sure to return at least the key_columns and hierarchy_cols for the rest of the script to work)\n",
    "def my_filter_func(cf_unified):\n",
    "    return cf_unified[cf_unified['unified.diagnosis_best'] == 'emcs']\n",
    "filter_func = my_filter_func\n",
    "\n",
    "# Hierarchy columns\n",
    "# define here what hierarchies should be used to create the subdirectory trees, in the order of the list (ie, 1st column's values will be top parent, then 2nd column's values is subdirectory, then 3rd column is subsubdirectory, etc)\n",
    "hierarchy_cols = ['unified.episedationsimple', 'unified.etiology']\n",
    "# prepend column name before the value in the folder name (be careful that the output filepath does not get too long, or you might run into errors!)\n",
    "hierarchy_prepend_colname = False\n",
    "# In case the value for a field is missing (for the hierarchy columns), what should we replace it with?\n",
    "placeholder_value = 'unknown'\n",
    "\n",
    "# Output folder for converted NIFTI files (a subfolder for each key will be created)\n",
    "output_dir = r'C:\\git\\niftis_reorg'\n",
    "\n",
    "# Skip conversion errors?\n",
    "skip_errors = True\n",
    "# Cleanup names to replace accentuated and special characters? (advised, please use same setting as in dicoms_to_nifti.ipynb)\n",
    "clean_names = True\n",
    "\n",
    "# Special parameters\n",
    "verbose = False\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv db as dataframe\n",
    "import pandas as pd\n",
    "\n",
    "cf_unified = pd.read_csv(unified_csv, sep=';', low_memory=False).dropna(axis=0, how='all').fillna('')  # drop empty lines\n",
    "cf_unified = df_to_unicode_fast(cf_unified, progress_bar=True)  # convert to unicode (can fix issues with accentuated characters)\n",
    "cf_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subset with non empty key columns and dicom column (ie, dicom is available)\n",
    "cf_unified_nonempty = cf_unified[~(cf_unified[key_columns].isnull() | (cf_unified[key_columns] == '')).any(axis=1)]\n",
    "cf_unified_nonempty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an id for each subject/session (will be used as the output folder name)\n",
    "# from dicoms_to_nifti.ipynb\n",
    "# TODO: make a function in aux.py to be shared\n",
    "def df_concat_cols(x):\n",
    "    \"\"\"Concatenate values over different columns\"\"\" \n",
    "    return '_'.join(x).strip().replace(' ','-')\n",
    "\n",
    "idcol = df_concat_cols(key_columns)\n",
    "cf_unified_nonempty.loc[:, idcol] = cf_unified_nonempty.loc[:, key_columns].apply(df_concat_cols, axis=1)\n",
    "if clean_names:\n",
    "    cf_unified_nonempty.loc[:, idcol] = cf_unified_nonempty.loc[:, idcol].apply(cleanup_name).apply(lambda x: x.replace(' ', '_'))\n",
    "cf_clean = cf_unified_nonempty[~cf_unified_nonempty[idcol].isnull()]\n",
    "cf_clean[idcol]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to keep only the rows we are interested in\n",
    "cf_filtered = filter_func(cf_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAIN LOOP\n",
    "# Copying the nifti folders with the generated hierarchy\n",
    "\n",
    "conflicts = []\n",
    "missing = []\n",
    "# For each row\n",
    "for idx, row in _tqdm(cf_filtered.iterrows(), total=len(cf_filtered), desc='REORG', unit='sessions'):\n",
    "    # Build the input and output paths\n",
    "    input_filepath = os.path.join(input_dir, row[idcol])\n",
    "    if not os.path.exists(input_filepath):\n",
    "        # Missing input file, we skip!\n",
    "        missing.append(input_filepath)\n",
    "    else:\n",
    "        # Organize per the specified hierarchy\n",
    "        outpath = []\n",
    "        for hcol in hierarchy_cols:  # select the columns to use as hierarchy\n",
    "            try:\n",
    "                # Get the value for this column\n",
    "                v = row[hcol]\n",
    "                # If empty, raise an error\n",
    "                if not v.strip():\n",
    "                    raise Exception('empty value')\n",
    "            except Exception as exc:\n",
    "                # If error (value empty or inexistent), we use a placeholder value\n",
    "                if verbose:\n",
    "                    print('Warning: no or empty value for hierarchical column %s for row id %s' % (hcol, row[idcol]))\n",
    "                v = placeholder_value\n",
    "            # Prepend the column name if option enabled\n",
    "            if hierarchy_prepend_colname:\n",
    "                v = '%s_%s' % (hcol, v)\n",
    "            # Add the value to the list of subfolders\n",
    "            outpath.append(v)\n",
    "        # Append the subject name as the final subfolder\n",
    "        outpath.append(row[idcol])\n",
    "        # Build the final path, prepending the output directory\n",
    "        output_filepath = os.path.join(output_dir, *outpath)\n",
    "        # Check if there is a conflict (output already exists)\n",
    "        if os.path.exists(output_filepath):\n",
    "            conflicts.append([input_filepath, output_filepath])\n",
    "        # Copy recursively!\n",
    "        copy_any(input_filepath, output_filepath)\n",
    "        # Debug stuff\n",
    "        if debug:\n",
    "            break\n",
    "\n",
    "print('All done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "if missing:\n",
    "    with open('niftis_missing.txt', 'w') as f:\n",
    "        f.write(pprint.pformat(missing, indent=4, width=80))\n",
    "    print('\\nSome nifti folders were not found, the list is saved in niftis_unprocessed.txt')\n",
    "else:\n",
    "    print('\\nAll nifti folders were processed!')\n",
    "if conflicts:\n",
    "    with open('niftis_conflicts.txt', 'w') as f:\n",
    "        f.write(pprint.pformat(conflicts, indent=4, width=80))\n",
    "    print('\\nSome nifti folders were in conflicts and got overwritten, the list is saved in niftis_conflicts.txt')\n",
    "else:\n",
    "    print('\\nNo conflicts found!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the subset of selected entries into demographics csv files\n",
    "cf_filtered_unicode = df_to_unicode_fast(cf_filtered)\n",
    "cf_filtered_extended = cf_unified[cf_unified[key_columns[0]].isin(cf_filtered_unicode[key_columns[0]])]  # TODO: also try to disambiguate names so that entries like DUPONT and dupont are also kept in this list\n",
    "cf_filtered_extended_unicode = df_to_unicode_fast(cf_filtered_extended)\n",
    "if save_df_as_csv(cf_filtered_unicode, unified_csv[:-4]+'_reorganizedsubset.csv', fields_order=False, csv_order_by=key_columns, date_format='%Y-%m-%d'):\n",
    "    save_df_as_csv(cf_filtered_extended_unicode, unified_csv[:-4]+'_reorganizedsubsetextended.csv', fields_order=False, csv_order_by=key_columns, date_format='%Y-%m-%d')\n",
    "    print('Subset demographics for the reorganized database successfully saved in %s and %s!' % (unified_csv[:-4]+'_reorganizedsubset.csv', unified_csv[:-4]+'_reorganizedsubsetextended.csv'))\n",
    "else:\n",
    "    print('ERROR: the subset demographics for the reorganized database could not be saved!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save extended subset of selected entries into demographics csv files\n",
    "# by first disambiguate any similar name, and then save any entry that can be relevant\n",
    "# this way, we make sure we don't miss any oddly named (eg, typo in name) entries\n",
    "cf_filtered_extended2_mapping = merge_two_df(cf_unified, cf_unified, col=key_columns[0], returnmerged=False, skip_sanity=True)\n",
    "cf_filtered_extended2_mapping2 = cf_filtered_extended2_mapping.loc[cf_filtered_extended2[key_columns[0]].isin(cf_filtered[key_columns[0]].unique()), :]\n",
    "cf_filtered_extended2 = cf_unified.loc[cf_unified[key_columns[0]].isin(cf_filtered_extended2_mapping2[key_columns[0]]) | cf_unified[key_columns[0]].isin(cf_filtered_extended2_mapping2[key_columns[0]+'2']), :]\n",
    "cf_filtered_extended2_unicode = df_to_unicode_fast(cf_filtered_extended2)\n",
    "if save_df_as_csv(cf_filtered_extended2_unicode, unified_csv[:-4]+'_reorganizedsubsetextended2.csv', fields_order=False, csv_order_by=key_columns, date_format='%Y-%m-%d'):\n",
    "    print('Subset demographics for the reorganized database successfully saved in %s!' % (unified_csv[:-4]+'_reorganizedsubsetextended2.csv'))\n",
    "else:\n",
    "    print('ERROR: the extended subset demographics for the reorganized database could not be saved!')\n",
    "# Display the entries\n",
    "cf_filtered_extended2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
