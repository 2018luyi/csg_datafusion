{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DICOMs to NIfTI converter\n",
    "By Stephen Larroque @ Coma Science Group, GIGA Research, University of Liege\n",
    "Creation date: 2019-03-08\n",
    "License: MIT\n",
    "v1.1.1\n",
    "\n",
    "DESCRIPTION:\n",
    "An agnostic DICOM folders and zip files converter into nifti, using a specific naming scheme synchronized with a demographics file, so as to ease subsequent manipulation of NIfTI dataset according to demographics data.\n",
    "\n",
    "Any command/converter can be used, this tool only provide the framework to automate the conversion process on lots of folders/zip files (and does a bit of sanity checks too).\n",
    "\n",
    "Note: a good alternative to this script is to use dcm2niix directly, but it works only on DICOM folders (not zips). Example of commandline: `dcm2niix.exe -f %s_%p_%d_%z/%s_%n_%t_%p_%d_%z_%r -d 9 -o output_dir input_dir`\n",
    "\n",
    "INSTALL NOTE:\n",
    "You need to pip install pandas before launching this script.\n",
    "Tested on Python 2.7.15\n",
    "You need also mcverter, as part of [MRIConvert](https://lcni.uoregon.edu/downloads/mriconvert).\n",
    "\n",
    "USAGE:\n",
    "Input: the final unified and postprocessed database (`merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall.csv`), resulting from using [csg_datafusion_finaldbunification.ipynb](csg_datafusion_finaldbunification.ipynb). The DICOM folders/zip files path should be included in the demographics file. If not, please use [csg_datafusion_dicoms_extract.ipynb](csg_datafusion_dicoms_extract.ipynb) to generate a new database (and merge it using [csg_datafusion_db_merger.ipynb](csg_datafusion_db_merger.ipynb)). If you have issues with DICOMs duplicates/conflicts, please use [csg_datafusion_dicoms_reorganizer.ipynb](csg_datafusion_dicoms_reorganizer.ipynb).\n",
    "\n",
    "TODO:\n",
    "* When an error happens, try to get back the original row from original database, and save as new csv, so that retrying the failed entries is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forcefully autoreload all python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUX FUNCTIONS\n",
    "\n",
    "import os, sys\n",
    "\n",
    "cur_path = os.path.realpath('.')\n",
    "sys.path.append(os.path.join(cur_path, 'csg_fileutil_libs'))  # for unidecode and cleanup_name, because it does not support relative paths (yet?)\n",
    "\n",
    "import re\n",
    "import shutil\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "from tempfile import mkdtemp\n",
    "\n",
    "from csg_fileutil_libs.aux_funcs import save_df_as_csv, _tqdm, reorder_cols_df, find_columns_matching, cleanup_name, df_to_unicode, df_to_unicode_fast, cleanup_name_df, df_literal_eval, reorder_cols_df, create_dir_if_not_exist, df_literal_eval, get_list_of_folders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "\n",
    "# Unified post-processed demographics database\n",
    "unified_csv = r'databases_output\\merged_fmp_steph_manon_sarah_dicom_ecg_reports_unifiedall.csv'\n",
    "# Columns in the unified demographics database to use as keys and also as naming for each converted DICOM folder/zip file\n",
    "# Note 1: rows will be filtered if any of these key columns is empty\n",
    "# Note 2: the resulting rows need to be unique: not any two rows should have the same key columns (all combined)\n",
    "# Note 3: later for automatic dataset reorganization, you will need to input the same key columns\n",
    "key_columns = ['name', 'StudyDate']\n",
    "# Column in unified_csv that contain the path to the DICOM file/folder\n",
    "dicom_column = 'dicom.path'\n",
    "# Replace part of the dicom path with a new path (eg if the path changed since generating the database). If None, no replacement will happen. 3rd value specify if using regular expression (True).\n",
    "dicom_path_replace = None # [r'H:\\\\ALLDICOMS\\\\PATIENTS\\\\', r'C:\\\\git\\\\datatest\\\\', True]\n",
    "# Command template to launch to convert. You can use whatever you want, just make sure to include the variables %(inputpath)s and %(outputpath)s\n",
    "# Note: don't forget to double % if it's part of the command and not meant to be replaced by a Python variable (eg, instead of %t, use %%t)\n",
    "# Note2: in the default dcm2niix command, the patient's details are anonymized. If you want to change that, set -ba n -t y\n",
    "cmd_template = r'C:\\git\\dcm2niix_11-Apr-2019_win\\dcm2niix.exe -z 3 -f %%s_%%p_%%d_%%z/%%s_%%t_%%p_%%d_%%z -d 9 -b y -ba y -t n -w 2 -o %(outputpath)s %(inputpath)s'\n",
    "\n",
    "# Output folder for converted NIFTI files (a subfolder for each key will be created)\n",
    "# WARNING: please check the output path does not exist before launching this script, to avoid any conflict!\n",
    "output_dir = r'G:\\hyperc_doc\\niftis2kpacsadd'\n",
    "\n",
    "# Skip conversion errors?\n",
    "skip_errors = True\n",
    "# Cleanup names to replace accentuated and special characters? (advised)\n",
    "clean_names = True\n",
    "\n",
    "# Special parameters\n",
    "verbose = False\n",
    "debug = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the csv db as dataframe\n",
    "import pandas as pd\n",
    "\n",
    "cf_unified = pd.read_csv(unified_csv, sep=';', low_memory=False).dropna(axis=0, how='all').fillna('')  # drop empty lines\n",
    "cf_unified = df_to_unicode_fast(cf_unified, progress_bar=True)  # convert to unicode (can fix issues with accentuated characters)\n",
    "cf_unified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract subset with non empty key columns and dicom column (ie, dicom is available)\n",
    "cf_unified_dicoms = cf_unified[~(cf_unified[key_columns + [dicom_column]].isnull() | (cf_unified[key_columns + [dicom_column]] == '')).any(axis=1)]\n",
    "cf_unified_dicoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All patients acquired on the same day, just out of curiosity\n",
    "cf_unified_dicoms[cf_unified_dicoms.duplicated(subset=[key_columns[-1]], keep=False)].sort_values(by=[key_columns[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Find any duplicated keys or dicom path, if that's the case we have an issue and we stop here\n",
    "cf_duplicated_keys = cf_unified_dicoms[cf_unified_dicoms.duplicated(subset=key_columns, keep=False)]\n",
    "cf_duplicated_dicompath = cf_unified_dicoms[cf_unified_dicoms.duplicated(subset=[dicom_column], keep=False)]\n",
    "if len(cf_duplicated_keys) or len(cf_duplicated_dicompath):\n",
    "    if len(cf_duplicated_keys):\n",
    "        print('ERROR: rows with duplicated keys were found! Please ensure your key columns are unique!')\n",
    "        duprows = cf_duplicated_keys[key_columns + [dicom_column]].sort_values(by=key_columns)\n",
    "        print(duprows)\n",
    "        if save_df_as_csv(duprows, 'duprows_keys.csv', fields_order=False):\n",
    "            print('List of duplicated rows were saved in duprows_keys.csv, please fix the dicom manually or change the key columns you use to ensure they are unique.')\n",
    "    if len(cf_duplicated_dicompath):\n",
    "        print('ERROR: multiple entries share the same dicom path! Please ensure the dicom path is unique for each entry!')\n",
    "        duprows = cf_duplicated_dicompath[key_columns + [dicom_column]].sort_values(by=dicom_column)\n",
    "        print(duprows)\n",
    "        if save_df_as_csv(duprows, 'duprows_dicompath.csv', fields_order=False):\n",
    "            print('List of duplicated rows were saved in duprows_dicompath.csv, please fix them manually (by moving dicoms in separate folders) before restarting this script.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace dicom paths if provided a replacement\n",
    "if dicom_path_replace:\n",
    "    cf_unified_dicoms.loc[:, dicom_column] = cf_unified_dicoms[dicom_column].str.replace(dicom_path_replace[0], dicom_path_replace[1], case=False, regex=dicom_path_replace[2])\n",
    "    cf_unified_dicoms[dicom_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate literals in dicom paths (in case there are multiple paths in a list/set)\n",
    "cf_unified_dicoms.loc[:, dicom_column] = cf_unified_dicoms.loc[:, dicom_column].apply(df_literal_eval)\n",
    "cf_unified_dicoms[dicom_column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create an id for each subject/session (will be used as the output folder name)\n",
    "def df_concat_cols(x):\n",
    "    \"\"\"Concatenate values over different columns\"\"\" \n",
    "    return '_'.join(x).strip().replace(' ','-')\n",
    "\n",
    "idcol = df_concat_cols(key_columns)\n",
    "# Clean names + select the key columns\n",
    "if clean_names:\n",
    "    # Cleanup name per column (else we might get buggy spaces if we apply on concatenated idcol)\n",
    "    cf_unified_keycols = cf_unified_dicoms.loc[:, key_columns].applymap(lambda name: cleanup_name(name))\n",
    "else:\n",
    "    cf_unified_keycols = cf_unified_dicoms.loc[:, key_columns]\n",
    "# Merge key columns in one (concatenated with an underscore)\n",
    "cf_unified_dicoms.loc[:, idcol] = cf_unified_keycols.apply(df_concat_cols, axis=1)\n",
    "# Clean name again\n",
    "if clean_names:\n",
    "    cf_unified_dicoms.loc[:, idcol] = cf_unified_dicoms.loc[:, idcol].apply(cleanup_name).apply(lambda x: x.replace(' ', '_'))\n",
    "# Make a new DataFrame with only the clean id column and dicom column\n",
    "cf_clean = cf_unified_dicoms[[idcol, dicom_column]]\n",
    "cf_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG\n",
    "# Test if lists and zip files are supported\n",
    "if debug:\n",
    "    cf_clean.loc[0, dicom_column] = [cf_clean.loc[0, dicom_column], '%s%s' % (cf_clean.loc[0, dicom_column],'.zip')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# MAIN CONVERSION LOOP\n",
    "def launch_cmd(inputpath, outputpath, robust=False, verbose=False):\n",
    "    \"\"\"Launch a command and return the output\"\"\"\n",
    "    cmd = cmd_template % {'inputpath': inputpath, 'outputpath': outputpath}\n",
    "    if verbose:\n",
    "        print(cmd)\n",
    "    try:\n",
    "        res = subprocess.check_output(cmd, shell=True)\n",
    "        if not res:\n",
    "            print('Error when converting:')\n",
    "            print(res)\n",
    "        return res\n",
    "    except Exception as exc:\n",
    "        if not robust:\n",
    "            raise exc\n",
    "        else:\n",
    "            print('ERROR: the command returned an error, the DICOM acquisition might have been partially or not at all converted to NIfTI, please check manually:')\n",
    "            print(exc)\n",
    "            pass\n",
    "            return -1\n",
    "    return 0\n",
    "\n",
    "missing = []\n",
    "conflicts = []\n",
    "errorslist = []\n",
    "# For each row\n",
    "for idx, row in _tqdm(cf_clean.iterrows(), total=len(cf_clean), desc='CONVERT', unit='sessions'):\n",
    "    # Build the input and output paths\n",
    "    input_filepaths = row[dicom_column]\n",
    "    output_filepath = os.path.join(output_dir, row[idcol])\n",
    "    if not isinstance(input_filepaths, (list, set)):\n",
    "        input_filepaths = [input_filepaths]\n",
    "    # If DICOM path for this row contains multiple paths (there are multiple DICOMs for this subject/session, might be duplicates or the acquisition was split in several DICOM directories/zip files), we loop on all of them\n",
    "    for input_filepath in input_filepaths:\n",
    "        # If the dicom path exists (as specified in the dataframe row)\n",
    "        if os.path.exists(input_filepath):\n",
    "            # If the output file already exists, then there is a conflict, we will overwrite but we save in a list all conflicts\n",
    "            if os.path.exists(output_filepath):\n",
    "                conflicts.append(input_filepath)\n",
    "            # Create the output folder for this row\n",
    "            create_dir_if_not_exist(output_filepath)\n",
    "            rtncode = 0\n",
    "            if not input_filepath.endswith('.zip'):\n",
    "                # If not a zip, we can directly process the whole folder\n",
    "                rtncode = launch_cmd(input_filepath, output_filepath, robust=skip_errors, verbose=verbose)\n",
    "            else:\n",
    "                # Else it is a zip, we first need to unzip it\n",
    "                temp_dir = mkdtemp()\n",
    "                try:\n",
    "                    # Unzip the dicoms into the temporary folder\n",
    "                    ziph = zipfile.ZipFile(input_filepath, 'r')\n",
    "                    ziph.extractall(temp_dir)\n",
    "                    ziph.close()\n",
    "\n",
    "                    # Convert\n",
    "                    rtncode = launch_cmd(temp_dir, output_filepath, robust=skip_errors, verbose=verbose)\n",
    "                finally:\n",
    "                    # Finally we delete the temporary directory\n",
    "                    shutil.rmtree(temp_dir)\n",
    "            if rtncode == -1:\n",
    "                errorslist.append([input_filepath, output_filepath])\n",
    "        else:\n",
    "            # The input dicom path does not exist, we have a missing dicom, we cannot convert (but save in a list for manual inspection)\n",
    "            missing.append(input_filepath)\n",
    "    if debug:\n",
    "        break\n",
    "\n",
    "print('All done!')\n",
    "if missing:\n",
    "    print('\\n')\n",
    "    print('Some input DICOMs were missing:')\n",
    "    print(missing)\n",
    "if conflicts:\n",
    "    print('\\n')\n",
    "    print('Some conflicts (probably duplicates) were found and overwritten, but please make sure to review the list below. If you have an issue, check the key_columns you set ensures no loss/mixing of DICOMs by using uniquely identifying key_colums! Here is the list of conflicts:')\n",
    "    print(conflicts)\n",
    "    print('Note: these were reported as conflicts because the output folder already existed before. There might not have been any overwriting if for example a subject examination was split in two, but with the same name and studydate, then the sequences will have a different id and everything is fine, but please review manually to ensure that.')\n",
    "if errorslist:\n",
    "    print('\\n')\n",
    "    print('Some DICOM folders could not be completely processed, the result might be partial or inexistent, please check manually the following entries:')\n",
    "    for (inp, outp) in errorslist:\n",
    "        print('* Input: %s -> Output: %s' % (inp, outp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: check each output nifti subfolder if there is something inside, else the subject was not converted at all (dcm2niix crash)\n",
    "empty_conversions = []\n",
    "for p in get_list_of_folders(output_dir):\n",
    "    if not len(os.listdir(os.path.join(output_dir, p))):\n",
    "        empty_conversions.append(p)\n",
    "\n",
    "if empty_conversions:\n",
    "    print('Some subjects/sessions could not be converted at all (probably because of converter crashing silently), here is the list:')\n",
    "    for e in empty_conversions:\n",
    "        print('* %s' % e)\n",
    "    print('Please just retry converting these subjects/sessions manually, sometimes this fixes the issue.')\n",
    "else:\n",
    "    print('All subjects/sessions could be partially or completely converted, congratulations!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
